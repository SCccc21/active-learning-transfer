# -*- coding: utf-8 -*-
"""cifar2stsl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndowz7Z2oJ7gIUtr0Cw8r9GOu5y3F4Pf

# Import Libraries
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR, MultiStepLR

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

import keras
from keras.layers import Input, Flatten, Dense, LeakyReLU, Dropout
from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers.core import Activation, Flatten, Dense
from keras import backend as K
from keras.models import clone_model
from keras import Model
from keras.datasets import mnist
from keras.utils import np_utils

import tensorflow.compat.v1 as tf
from tensorflow.keras import regularizers
from tensorflow.keras import initializers

import numpy as np
import collections
import copy
import pandas as pd 
import matplotlib.pyplot as plt 
import scipy.optimize as op 
import seaborn as sns

import pickle


prefix = '/content/gdrive/My Drive/'

"""### Sample Utility"""

# utility_func_args = [x_train, y_train, x_val, y_val]
def sample_utility(n, size_min, size_max, utility_func, utility_func_args, random_state, ub_prob=0.2, verbose=False):

  x_train, y_train, x_val, y_val = utility_func_args

  X_feature_test = []
  y_feature_test = []

  x_train = np.array(x_train)
  y_train = np.array(y_train)

  N = len(y_train)

  np.random.seed(random_state)
  
  for i in range(n):
    if verbose:
      print('{} / {}'.format(i, n))

    n_select = np.random.choice(range(size_min, size_max))

    subset_index = []

    """
    if unbalance:
      n_per_class = int(N / 10)
      alpha = np.ones(10)
      alpha[np.random.choice(range(10))] = np.random.choice(range(1, 50))
    else:
      alpha = np.random.choice(range(1, 20), size=10, replace=True)
    """

    toss = np.random.uniform()

    # With probability ub_prob, sample a class-imbalanced subset
    if toss > 1-ub_prob:
      n_per_class = int(N / 10)
      alpha = np.ones(10)
      alpha[np.random.choice(range(10))] = np.random.choice(range(1, 50))
    else:
      alpha = np.random.choice(range(90, 100), size=10, replace=True)

    p = np.random.dirichlet(alpha=alpha)
    occur = np.random.choice(range(10), size=n_select, replace=True, p=p)
    counts = np.array([np.sum(occur==i) for i in range(10)])
    
    for i in range(10):
      ind_i = np.where(np.argmax(y_train, 1)==i)[0]
      if len(ind_i) > counts[i]:
        selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=False)
      else:
        selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=True)
      subset_index = subset_index + list(selected_ind_i)

    subset_index = np.array(subset_index)

    y_feature_test.append(utility_func(x_train[subset_index], y_train[subset_index], x_val, y_val))
    X_feature_test.append( subset_index )

  return X_feature_test, y_feature_test


def sample_utility_veryub(n, size_min, size_max, utility_func, utility_func_args, random_state, ub_prob=0.2, verbose=False):

  x_train, y_train, x_val, y_val = utility_func_args

  X_feature_test = []
  y_feature_test = []

  x_train = np.array(x_train)
  y_train = np.array(y_train)

  N = len(y_train)

  np.random.seed(random_state)
  
  for i in range(n):
    if verbose:
      print('{} / {}'.format(i, n))

    n_select = np.random.choice(range(size_min, size_max))

    if n_select > 0:
      subset_index = []

      toss = np.random.uniform()

      # With probability ub_prob, sample a class-imbalanced subset
      if toss > 1-ub_prob:
        alpha = np.random.choice(range(1, 100), size=10, replace=True)
      else:
        alpha = np.random.choice(range(90, 100), size=10, replace=True)

      p = np.random.dirichlet(alpha=alpha)
      occur = np.random.choice(range(10), size=n_select, replace=True, p=p)
      counts = np.array([np.sum(occur==i) for i in range(10)])
      
      for i in range(10):
        ind_i = np.where(np.argmax(y_train, 1)==i)[0]
        if len(ind_i) > counts[i]:
          selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=False)
        else:
          selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=True)
        subset_index = subset_index + list(selected_ind_i)

      subset_index = np.array(subset_index)

      y_feature_test.append(utility_func(x_train[subset_index], y_train[subset_index], x_val, y_val))
      X_feature_test.append( subset_index )
    else:
      y_feature_test.append(0.1)
      X_feature_test.append( np.array([]) )

  return X_feature_test, y_feature_test

"""### Deep Sets"""

class DeepSet(nn.Module):

    def __init__(self, in_features, set_features=128, hidden_ext=128, hidden_reg=128):
        super(DeepSet, self).__init__()
        self.in_features = in_features
        self.out_features = set_features
        self.feature_extractor = nn.Sequential(
            nn.Linear(in_features, hidden_ext, bias=False),
            nn.ELU(inplace=True),
            nn.Linear(hidden_ext, hidden_ext, bias=False),
            nn.ELU(inplace=True),
            nn.Linear(hidden_ext, set_features, bias=False)
        )

        self.regressor = nn.Sequential(
            nn.Linear(set_features, hidden_reg, bias=False),
            nn.ELU(inplace=True),
            nn.Linear(hidden_reg, hidden_reg, bias=False),
            nn.ELU(inplace=True),
            nn.Linear(hidden_reg, int(hidden_reg/2), bias=False),
            nn.ELU(inplace=True)
        )

        self.linear = nn.Linear(int(hidden_reg/2), 1)
        self.sigmoid = nn.Sigmoid()
        
        self.add_module('0', self.feature_extractor)
        self.add_module('1', self.regressor)
        
    def reset_parameters(self):
        for module in self.children():
            reset_op = getattr(module, "reset_parameters", None)
            if callable(reset_op):
                reset_op()
            
    def forward(self, input):
        x = input
        x = self.feature_extractor(x)
        x = x.sum(dim=1)
        x = self.regressor(x)
        x = self.linear(x)
        x = self.sigmoid(x)
        return x

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'Feature Exctractor=' + str(self.feature_extractor) \
            + '\n Set Feature' + str(self.regressor) + ')'


class DeepSet_cifar(nn.Module):

    def __init__(self, in_features, set_features=512):
        super(DeepSet_cifar, self).__init__()
        self.in_features = in_features
        self.out_features = set_features
        self.feature_extractor = nn.Sequential(
            nn.Linear(in_features, 512),
            nn.ELU(inplace=True),
            nn.Linear(512, 512),
            nn.ELU(inplace=True),
            nn.Linear(512, set_features)
        )

        self.regressor = nn.Sequential(
            nn.Linear(set_features, 512),
            nn.ELU(inplace=True),
            nn.Linear(512, 512),
            nn.ELU(inplace=True),
            nn.Linear(512, 512),
            nn.ELU(inplace=True),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )
        
        self.add_module('0', self.feature_extractor)
        self.add_module('1', self.regressor)
        
        
    def reset_parameters(self):
        for module in self.children():
            reset_op = getattr(module, "reset_parameters", None)
            if callable(reset_op):
                reset_op()
            
    def forward(self, input):
        x = input
        x = self.feature_extractor(x)
        x = x.sum(dim=1)
        x = self.regressor(x)
        return x

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'Feature Exctractor=' + str(self.feature_extractor) \
            + '\n Set Feature' + str(self.regressor) + ')'

"""### Deepset Utility Learning Model"""

class Utility_deepset(object):

    def __init__(self, model=None):

        """
        if model is None:
          self.model = DeepSet(in_dims).cuda()
        else:
          self.model = model.cuda()
        """

        self.model = model

        self.model.linear.bias = torch.nn.Parameter(torch.tensor([-2.1972]))
        self.model.linear.bias.requires_grad = False
        #print(self.model.linear.bias)
        self.model.cuda()
        #print(self.model.linear.bias)
        
        self.l1 = nn.L1Loss()
        self.l2 = nn.MSELoss(reduction='sum')
        
    # train_data: x_train_few
    # train_set: (X_feature, y_feature)
    # valid_set: (X_feature_test, y_feature_test)
    def fit(self, train_data, train_set, valid_set, n_epoch, batch_size=32, lr=1e-3):

        self.optim = optim.Adam(self.model.parameters(), lr)

        #scheduler = StepLR(self.optim, step_size=10, gamma=0.1)
        scheduler = MultiStepLR(self.optim, milestones=[10,15], gamma=0.1)

        train_data = copy.deepcopy(train_data)
        N = train_data.shape[0]
        k = train_data.shape[1]

        X_feature, y_feature = train_set
        X_feature_test, y_feature_test = valid_set
        train_size = len(y_feature)

        for epoch in range(n_epoch):

          # Shuffle training utility samples
          ind = np.arange(train_size, dtype=int)
          np.random.shuffle(ind)
          X_feature = [X_feature[i] for i in ind]
          y_feature = y_feature[ind]

          train_loss = 0
          start_ind = 0

          for j in range(train_size//batch_size):
            start_ind = j*batch_size
            batch_X, batch_y = [], []
            for i in range(start_ind, min(start_ind+batch_size, train_size)):

              b = np.zeros((N, k))
              if len(X_feature[i]) > 0:
                selected_train_data = train_data[X_feature[i]]
                b[:len(X_feature[i])] = selected_train_data

              batch_X.append( b )
              batch_y.append( [y_feature[i]] )

            batch_X = np.stack(batch_X)
            batch_X, batch_y = torch.FloatTensor(batch_X).cuda(), torch.FloatTensor(batch_y).cuda()

            self.optim.zero_grad()
            y_pred = self.model(batch_X)
            loss = self.l2(y_pred, batch_y)
            loss_val = np.asscalar(loss.data.cpu().numpy())
            loss.backward()
            self.optim.step()
            train_loss += loss_val
          train_loss /= train_size
          test_loss = self.evaluate(train_data, valid_set)
          scheduler.step()
          print('Epoch %s Train Loss %s Test Loss %s' % (epoch, train_loss, test_loss))
          # print(self.model.linear.bias)
    
    def evaluate(self, train_data, valid_set):

        N, k = train_data.shape
        X_feature_test, y_feature_test = valid_set

        test_size = len(y_feature_test)
        test_loss = 0

        for i in range(test_size):

            b = np.zeros((N, k))
            if len(X_feature_test[i]) > 0:
              selected_train_data = train_data[X_feature_test[i]]
              b[:len(X_feature_test[i])] = selected_train_data

            batch_X, batch_y = torch.FloatTensor(b).cuda(), torch.FloatTensor(y_feature_test[i:i+1]).cuda()
            batch_X, batch_y = batch_X.reshape((1, N, k)), batch_y.reshape((1, 1))
            y_pred = self.model(batch_X)

            loss = self.l2(y_pred, batch_y)
            loss_val = np.asscalar(loss.data.cpu().numpy())
            test_loss += loss_val
        test_loss /= test_size
        return test_loss


def array_to_lst(X_feature):

  if type(X_feature) == list:
    return X_feature

  X_feature = list(X_feature)
  for i in range(len(X_feature)):
    X_feature[i] = X_feature[i].nonzero()[0]
  return X_feature


def findMostValuableSample_deepset_greedy(model, unlabeled_set, target_size):

  model = model.cuda()

  N, input_dim = unlabeled_set.shape
  k = target_size

  selected_subset = np.zeros(N)
  selected_rank = []
  selected_data = np.zeros((N, input_dim))

  for i in range(k):
    print(i)
    maxIndex, maxVal = -1, -1
    prevUtility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())
    searchRange = np.where(selected_subset == 0)[0]
    for j in searchRange:
      selected_subset[j] = 1
      selected_data[j] = unlabeled_set[j]
      utility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())
      selected_subset[j] = 0
      selected_data[j] = np.zeros(input_dim)
      if utility - prevUtility > maxVal:
        maxIndex = j
        maxVal = utility - prevUtility
    selected_subset[maxIndex] = 1
    selected_rank.append(maxIndex)
    selected_data[maxIndex] = unlabeled_set[maxIndex]

  return selected_subset, selected_rank, selected_data


def findMostValuableSample_deepset_stochasticgreedy(model, unlabeled_set, target_size, epsilon, seed, verbose=False, debug=False, label=None):

  model = model.cuda()

  N, input_dim = unlabeled_set.shape
  k = target_size

  selected_subset = np.zeros(N)
  selected_rank = []

  R = int((N/k)*np.log(1/epsilon))
  
  print('Sample Size R={}'.format(R))

  if debug:
    R = 10
    print('Sample Size R={}'.format(R))

  np.random.seed(seed)
  selected_data = np.zeros((N, input_dim))

  for i in range(k):
    if verbose: print(i)
    
    maxIndex, maxVal = -1, -1

    prevUtility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())
    searchRange = np.where(selected_subset == 0)[0]

    if debug:
      print('Step {}, prevUtility={}'.format(i, prevUtility))

    if R < len(searchRange):
      searchRange = np.random.choice(searchRange, size=R, replace=False)

    for j in searchRange:
      selected_subset[j] = 1
      selected_data[j] = unlabeled_set[j]
      utility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())
      selected_subset[j] = 0
      selected_data[j] = np.zeros(input_dim)
      gain = (utility - prevUtility).cpu().detach().numpy()[0][0]

      if gain > maxVal:
        maxIndex = j
        maxVal = gain

      if debug:
        print('  Gain from {} is {}, label={}'.format(j, gain, label[j]))
        print('  maxIndex={}, maxVal={}, labelMaxIndex={}'.format(maxIndex, maxVal, label[maxIndex]))
      
    selected_subset[maxIndex] = 1
    selected_rank.append(maxIndex)
    selected_data[maxIndex] = unlabeled_set[maxIndex]

  return selected_subset, selected_rank, selected_data

"""### FASS"""

class PyTorchFlatten(nn.Module):
    def forward(self, input):
        return input.view(input.size(0), -1)

class LogisticRegression(torch.nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LogisticRegression, self).__init__()
        self.flatten = PyTorchFlatten()
        self.linear = torch.nn.Linear(input_dim, output_dim)
        torch.nn.init.xavier_uniform(self.linear.weight)
        torch.nn.init.zeros_(self.linear.bias)

    def forward(self, x, last=False):
        x = self.flatten(x)
        outputs = self.linear(x)
        if last:
            return outputs, x
        else:
            return outputs

    def get_embedding_dim(self):
        return 28*28


class MnistLeNet(torch.nn.Module):
    def __init__(self):
        super(MnistLeNet, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding_mode='replicate')
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.linear1 = nn.Linear(in_features=2880, out_features=500)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(in_features=500, out_features=10)

        torch.nn.init.xavier_uniform(self.conv1.weight)
        torch.nn.init.zeros_(self.conv1.bias)

        torch.nn.init.xavier_uniform(self.linear1.weight)
        torch.nn.init.zeros_(self.linear1.bias)

        torch.nn.init.xavier_uniform(self.linear2.weight)
        torch.nn.init.zeros_(self.linear2.bias)

    def forward(self, x, last=False):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = torch.flatten(x, 1)
        x = self.relu(self.linear1(x))
        logits = self.linear2(x)
        outputs = F.softmax(logits, dim=1)

        if last:
            return outputs, x
        else:
            return outputs

    def get_embedding_dim(self):
        return 500

from torch.utils.data import Dataset, DataLoader, TensorDataset

def torch_mnist_logistic_data_to_acc(x_train, y_train, verbose=0):

  criterion = torch.nn.CrossEntropyLoss()
  net = LogisticRegression(28*28, 10)
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-7)

  tensor_x, tensor_y = torch.Tensor(x_train), torch.Tensor(y_train)
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=32, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test), torch.Tensor(y_test)
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(5):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images.view(-1, 28 * 28))
          labels = Variable(labels)
          labels = torch.argmax(labels, dim=1)
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          images = Variable(images.view(-1, 28*28))
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          labels = torch.argmax(labels, dim=1)
          correct += (predicted == labels).sum()
      accuracy = 100 * correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  print('Model Accuracy: {}'.format(accuracy))
  return net

"""## Add Gaussian Noise"""

from google.colab.patches import cv2_imshow

def addGaussianNoise(x_train, scale=1):
  return np.clip(x_train+np.random.normal(scale=scale, size=x_train.shape), 0, 1)

"""## cyCADA"""

import torch
import torch.nn as nn
from torch.nn import init
import numpy as np

models = {}
def register_model(name):
    def decorator(cls):
        models[name] = cls
        return cls
    return decorator

def get_model(name, num_cls=10, **args):
    net = models[name](num_cls=num_cls, **args)
    if torch.cuda.is_available():
        net = net.cuda()
    return net

def init_weights(obj):
    for m in obj.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            init.xavier_normal_(m.weight)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
            m.reset_parameters()


class TaskNet(nn.Module):

    num_channels = 3
    image_size = 32
    name = 'TaskNet'

    "Basic class which does classification."
    def __init__(self, num_cls=10, weights_init=None):
        super(TaskNet, self).__init__()
        self.num_cls = num_cls
        self.setup_net()
        self.criterion = nn.CrossEntropyLoss()
        if weights_init is not None:
            self.load(weights_init)
        else:
            init_weights(self)

    def forward(self, x, with_ft=False):
        x = self.conv_params(x)
        x = x.view(x.size(0), -1)
        x = self.fc_params(x)
        score = self.classifier(x)
        if with_ft:
            return score, x
        else:
            return score

    def setup_net(self):
        """Method to be implemented in each class."""
        pass

    def load(self, init_path):
        net_init_dict = torch.load(init_path)
        self.load_state_dict(net_init_dict)

    def save(self, out_path):
        torch.save(self.state_dict(), out_path)

@register_model('LeNet')
class LeNet(TaskNet):
    "Network used for MNIST or USPS experiments."    

    num_channels = 1
    image_size = 28
    name = 'LeNet'
    out_dim = 500 # dim of last feature layer

    def setup_net(self):

        self.conv_params = nn.Sequential(
                nn.Conv2d(self.num_channels, 20, kernel_size=5),
                nn.MaxPool2d(2),
                nn.ReLU(),
                nn.Conv2d(20, 50, kernel_size=5),
                nn.Dropout2d(p=0.5),
                nn.MaxPool2d(2),
                nn.ReLU(),
                )
        
        self.fc_params = nn.Linear(50*4*4, 500)
        self.classifier = nn.Sequential(
                nn.ReLU(),
                nn.Dropout(p=0.5),
                nn.Linear(500, self.num_cls)
                )


@register_model('DTN')
class DTNClassifier(TaskNet):
    "Classifier used for SVHN->MNIST Experiment"

    num_channels = 3
    image_size = 32
    name = 'DTN'
    out_dim = 512 # dim of last feature layer

    def setup_net(self):
        self.conv_params = nn.Sequential (
                nn.Conv2d(self.num_channels, 64, kernel_size=5, stride=2, padding=2),
                nn.BatchNorm2d(64),
                nn.Dropout2d(0.1),
                nn.ReLU(),
                nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),
                nn.BatchNorm2d(128),
                nn.Dropout2d(0.3),
                nn.ReLU(),
                nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),
                nn.BatchNorm2d(256),
                nn.Dropout2d(0.5),
                nn.ReLU()
                )
    
        self.fc_params = nn.Sequential (
                nn.Linear(256*4*4, 512),
                nn.BatchNorm1d(512),
                )

        self.classifier = nn.Sequential(
                nn.ReLU(),
                nn.Dropout(),
                nn.Linear(512, self.num_cls)
                )

@register_model('AddaNet')
class AddaNet(nn.Module):
    "Defines and Adda Network."
    def __init__(self, num_cls=10, model='LeNet', src_weights_init=None,
            weights_init=None):
        super(AddaNet, self).__init__()
        self.name = 'AddaNet'
        self.base_model = model
        self.num_cls = num_cls
        self.cls_criterion = nn.CrossEntropyLoss()
        self.gan_criterion = nn.CrossEntropyLoss()
      
        self.setup_net()
        if weights_init is not None:
            self.load(weights_init)
        elif src_weights_init is not None:
            self.load_src_net(src_weights_init)
        else:
            raise Exception('AddaNet must be initialized with weights.')
        

    def forward(self, x_s, x_t):
        """Pass source and target images through their
        respective networks."""
        score_s, x_s = self.src_net(x_s, with_ft=True)
        score_t, x_t = self.tgt_net(x_t, with_ft=True)

        if self.discrim_feat:
            d_s = self.discriminator(x_s)
            d_t = self.discriminator(x_t)
        else:
            d_s = self.discriminator(score_s)
            d_t = self.discriminator(score_t)
        return score_s, score_t, d_s, d_t

    def setup_net(self):
        """Setup source, target and discriminator networks."""
        self.src_net = get_model(self.base_model, num_cls=self.num_cls)
        self.tgt_net = get_model(self.base_model, num_cls=self.num_cls)

        input_dim = self.num_cls 
        self.discriminator = nn.Sequential(
                nn.Linear(input_dim, 500),
                nn.ReLU(),
                nn.Linear(500, 500),
                nn.ReLU(),
                nn.Linear(500, 2),
                )

        self.image_size = self.src_net.image_size
        self.num_channels = self.src_net.num_channels

    def load(self, init_path):
        "Loads full src and tgt models."
        net_init_dict = torch.load(init_path)
        self.load_state_dict(net_init_dict)

    def load_src_net(self, init_path):
        """Initialize source and target with source
        weights."""
        self.src_net.load(init_path)
        self.tgt_net.load(init_path)

    def save(self, out_path):
        torch.save(self.state_dict(), out_path)

    def save_tgt_net(self, out_path):
        torch.save(self.tgt_net.state_dict(), out_path)

"""## Image Preprocess

### CIFAR
"""

def cifar_encoderProcess(x_train, y_train, x_test, y_test):

  N_train = len(y_train)
  N_test = len(y_test)


  # x_train = x_train.astype(np.float32)
  # x_test = x_test.astype(np.float32)
  # x_train = np.moveaxis(x_train, 1, 3)
  # x_test = np.moveaxis(x_test, 1, 3)

  x_train = torch.from_numpy(x_train)
  x_test = torch.from_numpy(x_test) # tensor of shape 3, 32, 32
  
  x_transfer = transforms.Compose([
                      transforms.ToPILImage(), #Tensor of shape C x H x W or a numpy ndarray of shape H x W x C
                      transforms.ToTensor(),
                      # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2430, 0.2610))
                      ])

  x_train_r = []
  for i in range(x_train.shape[0]):
    # import pdb; pdb.set_trace()
    x_r = x_transfer(x_train[i])
    x_train_r.append(x_r)

  b = torch.Tensor(N_train, 3, 32, 32)
  x_train_r = torch.cat(x_train_r, out=b)
  x_train_r = x_train_r.reshape(N_train, 3, 32, 32)

  x_test_r = []
  for i in range(x_test.shape[0]):
    x_r = x_transfer(x_test[i])
    x_test_r.append(x_r)

  b = torch.Tensor(N_test, 3, 32, 32)
  x_test_r = torch.cat(x_test_r, out=b)
  x_test_r = x_test_r.reshape(N_test, 3, 32, 32)

  y_train = torch.Tensor(y_train).long()
  y_test = torch.Tensor(y_test).long()

  return x_train_r, y_train, x_test_r, y_test

"""### CIFAR data to acc"""

import torchvision.datasets as datasets
import torchvision
import torchvision.transforms as transforms

class SmallCNN_CIFAR(nn.Module):
    def __init__(self):
        super(SmallCNN_CIFAR, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x, last=False):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        feat = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(feat))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        if last:
          return x, feat
        else:
          return x

    def getFeature(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        return x

    def get_embedding_dim(self):
        return 16 * 5 * 5



class LargeCNN(nn.Module):

    def __init__(self):
        super(LargeCNN, self).__init__()

        self.conv_layer = nn.Sequential(

            # Conv Layer block 1
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Conv Layer block 2
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(p=0.05),

            # Conv Layer block 3
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )


        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(4096, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(512, 10)
        )


    def forward(self, x):
        """Perform forward."""
        
        # conv layers
        x = self.conv_layer(x)
        
        # flatten
        x = x.view(x.size(0), -1)
        
        # fc layer
        x = self.fc_layer(x)

        return x

def torch_cifar_logistic_data_to_acc(x_train, y_train, verbose=0):

  criterion = torch.nn.CrossEntropyLoss()
  net = SmallCNN_CIFAR().cuda()
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-7)

  tensor_x, tensor_y = torch.Tensor(x_train).cuda(), torch.Tensor(y_train).cuda()
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=32, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test).cuda(), torch.Tensor(y_test).cuda()
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(10):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images)
          labels = Variable(labels).long()
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          images = Variable(images)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum()
      accuracy = 100 * correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  return accuracy.item()


def torch_cifar_logistic_data_to_net(x_train, y_train, x_test, y_test, n_epoch=10, verbose=0):

  criterion = torch.nn.CrossEntropyLoss()
  net = SmallCNN_CIFAR().cuda()
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-7)

  tensor_x, tensor_y = torch.Tensor(x_train).cuda(), torch.Tensor(y_train).cuda()
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=32, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test).cuda(), torch.Tensor(y_test).cuda()
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(n_epoch):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images)
          labels = Variable(labels).long()
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          images = Variable(images)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum()
      accuracy = 100 * correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  return net


def torch_cifar_logistic_data_to_acc_multiple(X_train, y_train, repeat=3, verbose=0):
  acc_lst = []
  for _ in range(repeat):
    acc = torch_cifar_logistic_data_to_acc(X_train, y_train, verbose)
    acc_lst.append(acc)
  return np.mean(acc_lst)




def torch_cifar_cnn_data_to_acc(x_train, y_train, verbose=0):

  criterion = torch.nn.CrossEntropyLoss()
  net = LargeCNN().cuda()
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-7)

  tensor_x, tensor_y = torch.Tensor(x_train).cuda(), torch.Tensor(y_train).cuda()
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=32, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test).cuda(), torch.Tensor(y_test).cuda()
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(20):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images)
          labels = Variable(labels).long()
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          images = Variable(images)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum()
      accuracy = 100 * correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  return accuracy.item()


def torch_cifar_cnn_data_to_acc_multiple(X_train, y_train, repeat=3, verbose=0):
  acc_lst = []
  for _ in range(repeat):
    acc = torch_cifar_cnn_data_to_acc(X_train, y_train, verbose)
    acc_lst.append(acc)
  return np.mean(acc_lst)

"""### STL"""

def stl_encoderProcess(x_train, y_train, x_test, y_test):

  N_train = len(y_train)
  N_test = len(y_test)


  # x_train = x_train.astype(np.float32)
  # x_test = x_test.astype(np.float32)
  # x_train = np.moveaxis(x_train, 1, 3)
  # x_test = np.moveaxis(x_test, 1, 3)

  x_train = torch.from_numpy(x_train)
  x_test = torch.from_numpy(x_test) # tensor of shape 3, 96, 96
  
  x_transfer = transforms.Compose([
                      transforms.ToPILImage(), #Tensor of shape C x H x W or a numpy ndarray of shape H x W x C
                      transforms.Resize(32),
                      transforms.ToTensor(),
                      # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                      transforms.Normalize((0.4467, 0.4398, 0.4066), (0.2603, 0.2565, 0.2712))
                      ])

  x_train_r = []
  for i in range(x_train.shape[0]):
    # import pdb; pdb.set_trace()
    x_r = x_transfer(x_train[i])
    x_train_r.append(x_r)

  b = torch.Tensor(N_train, 3, 32, 32)
  x_train_r = torch.cat(x_train_r, out=b)
  x_train_r = x_train_r.reshape(N_train, 3, 32, 32)

  x_test_r = []
  for i in range(x_test.shape[0]):
    x_r = x_transfer(x_test[i])
    x_test_r.append(x_r)

  b = torch.Tensor(N_test, 3, 32, 32)
  x_test_r = torch.cat(x_test_r, out=b)
  x_test_r = x_test_r.reshape(N_test, 3, 32, 32)

  y_train = torch.Tensor(y_train).long()
  y_test = torch.Tensor(y_test).long()

  return x_train_r, y_train, x_test_r, y_test

"""### STL data to acc"""

#NOTE: y_train should be one-hot vector


class SmallCnnSTL(nn.Module):
    def __init__(self):
        super(SmallCnnSTL, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 12, 5)
        self.fc1 = nn.Linear(12 * 441, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 12 * 441)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def getFeature(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1)
        return x


from torch.utils.data import Dataset, DataLoader, TensorDataset

def torch_stl_smallcnn_data_to_acc(x_train, y_train, x_test, y_test, verbose=0):

  if x_train.shape[1]==96:
    x_train = np.moveaxis(x_train, 3, 1)
    x_test = np.moveaxis(x_test, 3, 1)

  y_train = y_train.reshape(-1)
  y_test = y_test.reshape(-1)

  criterion = torch.nn.CrossEntropyLoss()
  net = SmallCnnSTL().cuda()
  optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, eps=1e-7)

  # import pdb;pdb.set_trace()
  tensor_x, tensor_y = torch.Tensor(x_train).cuda(), torch.Tensor(y_train).cuda()
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=16, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test).cuda(), torch.Tensor(y_test).cuda()
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(30):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images)
          labels = Variable(labels).long()
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          images = Variable(images)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum()
      accuracy = correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  return accuracy.item()


def torch_stl_smallcnn_data_to_acc_multiple(x_train, y_train, x_val, y_val, repeat=3, verbose=0):
  acc_lst = []
  for _ in range(repeat):
    acc = torch_stl_smallcnn_data_to_acc(x_train, y_train, x_val, y_val, verbose)
    acc_lst.append(acc)
  return np.mean(acc_lst)

"""## ResNetCifar"""

# Based on the ResNet implementation in torchvision
# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py

import math
import torch
from torch import nn
from torchvision.models.resnet import conv3x3

class BasicBlock(nn.Module):
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.downsample = downsample
        self.stride = stride
        
        self.bn1 = nn.BatchNorm2d(inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = conv3x3(inplanes, planes, stride)
        
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)

    def forward(self, x):
        residual = x 
        residual = self.bn1(residual)
        residual = self.relu1(residual)
        residual = self.conv1(residual)

        residual = self.bn2(residual)
        residual = self.relu2(residual)
        residual = self.conv2(residual)

        if self.downsample is not None:
            x = self.downsample(x)
        return x + residual

class Downsample(nn.Module):
    def __init__(self, nIn, nOut, stride):
        super(Downsample, self).__init__()
        self.avg = nn.AvgPool2d(stride)
        assert nOut % nIn == 0
        self.expand_ratio = nOut // nIn

    def forward(self, x):
        x = self.avg(x)
        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)

class ResNetCifar(nn.Module):
    def __init__(self, depth, width=1, block=BasicBlock, classes=10, channels=3):
        assert (depth - 2) % 6 == 0
        self.N = (depth - 2) // 6
        super(ResNetCifar, self).__init__()

        # Following the Wide ResNet convention, we fix the very first convolution
        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.inplanes = 16
        self.layer1 = self._make_layer(block, 16 * width)
        self.layer2 = self._make_layer(block, 32 * width, stride=2)
        self.layer3 = self._make_layer(block, 64 * width, stride=2)
        self.bn = nn.BatchNorm2d(64 * width)
        self.relu = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(8)
        self.fc = nn.Linear(64 * width, classes)

        # Initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
                
    def _make_layer(self, block, planes, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes:
            downsample = Downsample(self.inplanes, planes, stride)
        layers = [block(self.inplanes, planes, stride, downsample=downsample)]
        self.inplanes = planes
        for i in range(self.N - 1):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

"""# Transfer Experiment

## Source domain (CIFAR)

### Load sampled few data and utility training set (idx, utility)
"""

sample_dir = prefix + '/active-learning-transfer/samples/cifar10-samples'

with open(sample_dir+'/CIFAR_fewShot_N500_lo30_hi500_DataSeed903_Nsample5000.sample', 'rb') as f:
  X_feature_903, y_feature_903 = pickle.load(f)

with open(sample_dir+'/CIFAR_fewShot_N500_lo30_hi500_DataSeed903_Nsample1000_Seed4_UnBalanceTrue.sample', 'rb') as f:
  X_feature_ub_903, y_feature_ub_903 = pickle.load(f)

X_feature_903 = array_to_lst(X_feature_903) + array_to_lst(X_feature_ub_903)
y_feature_903 = np.concatenate((y_feature_903, y_feature_ub_903))

ind = np.arange(len(X_feature_903), dtype=int)
np.random.shuffle(ind)

X_feature_903 = [X_feature_903[i] for i in ind]
y_feature_903 = y_feature_903[ind]

if y_feature_903[0] > 1:
  y_feature_903 = y_feature_903 / 100

X_feature_test, y_feature_test = X_feature_903[5000:], y_feature_903[5000:]
X_feature_train, y_feature_train = X_feature_903[:5000], y_feature_903[:5000]


print(len(X_feature_train), y_feature_train.shape)
print(len(X_feature_test), y_feature_test.shape)

import torchvision.datasets as datasets
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose([transforms.ToTensor(), 
                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2430, 0.2610))
                                ])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)

x_train, y_train = trainset.data, np.array(trainset.targets)
x_test, y_test = testset.data, np.array(testset.targets)

x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

x_train = np.moveaxis(x_train, 3, 1)
x_test = np.moveaxis(x_test, 3, 1)

x_train_1d = x_train.reshape((x_train.shape[0], -1))
x_test_1d = x_test.reshape((x_test.shape[0], -1))

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)



N = 500

# Sample 30 data from each of label
x_train_few = np.zeros((50*10, 3, 32, 32))
y_train_few = np.zeros((50*10))

# Random Seed: 902, 903, 904
np.random.seed(903)
for i in range(10):
  idx = (y_train==i).nonzero()[0]
  idx_i = np.random.choice(idx, size=50, replace=False)
  x_train_few[i*50:(i+1)*50] = x_train[idx_i]
  y_train_few[i*50:(i+1)*50] = y_train[idx_i]

x_train_few_1d = x_train_few.reshape((x_train_few.shape[0], -1))

print(x_train_few.shape)
print(y_train_few.shape)
print(x_train_few_1d.shape)


N_val = 500
ind = np.random.choice(range(len(y_test)), size=N_val, replace=False)
x_val_few, y_val_few = x_test[ind], y_test[ind]
x_val_few_1d = x_val_few.reshape((x_val_few.shape[0], -1))
print(x_val_few_1d.shape)

# check if need to re (with normalization)
x_train_few_re, y_train_few_re, x_val_few_re, y_val_few_re = cifar_encoderProcess(x_train_few, y_train_few, x_val_few, y_val_few)

print(x_train_few.shape)
print(x_train_few_re.shape)

# get centriods of SYN
  # x_train_few_re, y_train_few_re
masks = []
cluster_syn = []
for k in range(10):
  mask = np.zeros_like(y_train_few_re, dtype=bool)
  for i in range(len(y_train_few_re)):
    mask[i] = True if y_train_few_re[i] == k else False 
  
  cluster_x = x_train_few_re[mask]
  if cluster_x.shape[0] > 1:
    centroid = featureExtract(cluster_x, extractor_from_layer3(net))
    centroid = torch.mean(centroid, dim=0)
  else:
    print('Unnormal Alert!')

  masks.append(mask)
  cluster_syn.append(centroid)

pickle.dump(cluster_syn, open(model_dir+'/cluster_source_cifar.data', 'wb'))

"""### Extract feature (cycada)"""

def featureExtract_cycda(x, ext):
  assert list(x.shape[1:]) == [1, ext.image_size, ext.image_size]
  score, out = ext(x.cuda(), with_ft=True)
  return out.view(out.size(0), -1)

model_dir = prefix + '/active-learning-transfer/models/cycada/ds/iter_1_28_noise'
adda_net_file = model_dir + '/adda_LeNet_net_mnist_usps.pth'
net = get_model('AddaNet', num_cls=10, weights_init=adda_net_file, 
                model='LeNet')
net.eval()

mnist_x_train_few_resNetFeature = featureExtract_cycda(mnist_x_train_few, net.src_net)
mnist_x_train_few_resNetFeature = mnist_x_train_few_resNetFeature.cpu().detach().numpy()

# val feature will be used when perform AL selection
mnist_x_val_few_resNetFeature = featureExtract_cycda(mnist_x_val_few, net.src_net)
mnist_x_val_few_resNetFeature = mnist_x_val_few_resNetFeature.cpu().detach().numpy()

"""### Extract feature (ResNet)"""

def extractor_from_layer3(net):
	layers = [net.conv1, net.layer1, net.layer2, net.layer3, net.bn, net.relu, net.avgpool]
	return nn.Sequential(*layers)
 
def featureExtract(x, ext):
  assert list(x.shape[1:]) == [3, 32, 32]
  out = ext(x.float().cuda())

  return out.view(out.size(0), -1)

model_dir = prefix + '/active-learning-transfer/models/cycada/cifar_to_stl_uda'
load_name = model_dir + "/ResNet_noise0.2_width2.pth"

depth = 26
width = 2 #16

net = ResNetCifar(depth, width, classes=10, channels=3).cuda()
net.load_state_dict(torch.load(load_name))
net.eval()

x_train_few_resNetFeature = featureExtract(torch.from_numpy(x_train_few), extractor_from_layer3(net))
x_train_few_resNetFeature = x_train_few_resNetFeature.cpu().detach().numpy()

# val feature will be used when perform AL selection
x_val_few_resNetFeature = featureExtract(torch.from_numpy(x_val_few), extractor_from_layer3(net))
x_val_few_resNetFeature = x_val_few_resNetFeature.cpu().detach().numpy()

print(x_train_few_re.shape)
print(x_train_few.shape)

"""### Train deepset"""

input_dim = x_train_few_resNetFeature.shape[1] # 最多2000
print("input dim:", input_dim)
set_features=256
hidden_ext=256
hidden_reg=256
n_epoch=20
batch_size=32
lr = 1e-5

dset = DeepSet(input_dim, set_features, hidden_ext, hidden_reg)
model = Utility_deepset(model=dset)
model.fit(x_train_few_resNetFeature, (X_feature, y_feature), (X_feature_test, y_feature_test), n_epoch, batch_size, lr)

print("Save model to path:{}?".format(model_dir))
import pdb; pdb.set_trace()
torch.save(model.model.state_dict(), model_dir+'/deepset_DataSeed903_width2.state_dict')

input_dim = x_train_few_resNetFeature.shape[1]
set_features=256
hidden_ext=256
hidden_reg=256
n_epoch=20
batch_size=32
lr = 1e-5

dset = DeepSet(input_dim, set_features, hidden_ext, hidden_reg)
model = Utility_deepset(model=dset)

# load_name = model_dir + '/deepset_DataSeed903.state_dict'
load_name = model_dir + '/deepset_DataSeed903_width2.state_dict'
model.model.load_state_dict(torch.load(load_name))

"""## Target Domain (STL)

### Sample target unlabeled
"""

transform = transforms.Compose([transforms.ToTensor(), 
                                transforms.Resize(32),
                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                transforms.Normalize((0.4467, 0.4398, 0.4066), (0.2603, 0.2565, 0.2712))
                                ])

trainset = torchvision.datasets.STL10(root='./data', split='train',
                                        download=True, transform=transform)
testset = torchvision.datasets.STL10(root='./data', split='test',
                                       download=True, transform=transform)

stl_x_train, stl_y_train = trainset.data, np.array(trainset.labels)
stl_x_test, stl_y_test = testset.data, np.array(testset.labels)

stl_x_train = stl_x_train.astype("float32") / 255.0
stl_x_test = stl_x_test.astype("float32") / 255.0



stl_x_train_1d = stl_x_train.reshape((stl_x_train.shape[0], -1))
stl_x_test_1d = stl_x_test.reshape((stl_x_test.shape[0], -1))

print(stl_x_train.shape)
print(stl_x_train.min(), stl_x_train.max())
print(stl_x_test.shape)

target_size = 1000
unlabel_size = 2000
unlaebl_seed = 2000 # 

np.random.seed(unlaebl_seed)
idx = np.random.choice(range(stl_x_train.shape[0]), size=unlabel_size, replace=False)

stl_x_unlabel_few, stl_y_unlabel_few = stl_x_train[idx], stl_y_train[idx]
stl_x_unlabel_few_1d = stl_x_unlabel_few.reshape((stl_x_unlabel_few.shape[0], -1))
print(stl_x_unlabel_few.shape)
print(stl_y_unlabel_few.shape)
print(stl_x_unlabel_few_1d.shape)

noise_frac = 0.25
n_noise_data = int(unlabel_size*noise_frac)
noise_scale = 10

stl_x_unlabel_few[:n_noise_data] = addGaussianNoise(stl_x_unlabel_few[:n_noise_data], scale=noise_scale)

#x_train_few_test[:n_noise_data] = addGaussianNoise(x_train_few_test[:n_noise_data], scale=1)
#x_train_few_test[n_noise_data:2*n_noise_data] = addGaussianNoise(x_train_few_test[n_noise_data:2*n_noise_data], scale=1)
#x_train_few_test[2*n_noise_data:3*n_noise_data] = addGaussianNoise(x_train_few_test[2*n_noise_data:3*n_noise_data], scale=1)

stl_x_unlabel_few_1d = stl_x_unlabel_few.reshape((stl_x_unlabel_few.shape[0], -1))
# stl_x_unlabel_few_channelLast = np.moveaxis(stl_x_unlabel_few, 1, 3) #NOTE check if need this
print(stl_x_unlabel_few_1d[0, :10])

stl_x_unlabel_few_re, stl_y_unlabel_few_re, stl_x_test_re, stl_y_test_re = stl_encoderProcess(stl_x_unlabel_few, stl_y_unlabel_few, stl_x_test, stl_y_test)
print(stl_x_unlabel_few_re.shape)

print(stl_x_unlabel_few.shape)
print(stl_x_unlabel_few_re.shape)

stl_x_unlabel_resNetFeature = featureExtract(stl_x_unlabel_few_re, extractor_from_layer3(net))
stl_x_unlabel_resNetFeature = stl_x_unlabel_resNetFeature.cpu().detach().numpy()

pickle.dump(stl_x_unlabel_resNetFeature, open(model_dir+'/stl_x_unlabel_resNetFeature.data', 'wb'))

"""### DULO"""

print(n_noise_data)

import time
start = time.time()
_, deepset_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model.model, stl_x_unlabel_resNetFeature, target_size, epsilon=1e-4, seed=100)
time.time() - start

input_dim = x_train_few_resNetFeature.shape[1] # 最多2000
print("input dim:", input_dim)
set_features=256
hidden_ext=256
hidden_reg=256
n_epoch=20
batch_size=32
lr = 1e-5
FLAG = False

while FLAG == False:
  dset = DeepSet(input_dim, set_features, hidden_ext, hidden_reg)
  model = Utility_deepset(model=dset)

  for e in range(n_epoch):
    print('n_epoch:', e)
    model.fit(x_train_few_resNetFeature, (X_feature_train, y_feature_train), (X_feature_test, y_feature_test), 1, batch_size, lr)
    _, deepset_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model.model, stl_x_unlabel_resNetFeature, target_size, epsilon=1e-1, seed=101)
    print(sum(np.array(deepset_selected_rank) < n_noise_data))
    if sum(np.array(deepset_selected_rank) < n_noise_data) < 30:
      FLAG = True
      torch.save(model.model.state_dict(), model_dir+'/deepset_{}_{}_{}_{}_{}.state_dict'.format(set_features,hidden_ext,hidden_reg, e, sum(np.array(deepset_selected_rank) < n_noise_data)))

input_dim = x_train_few_resNetFeature.shape[1] # 最多2000
print("input dim:", input_dim)
set_features=256
hidden_ext=256
hidden_reg=256
n_epoch=20
batch_size=32
lr = 1e-5
FLAG = False

dset = DeepSet(input_dim, set_features, hidden_ext, hidden_reg)
model = Utility_deepset(model=dset)
model.model.load_state_dict(torch.load(model_dir+'/deepset_256_256_256_6_28.state_dict'))

_, deepset_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model.model, stl_x_unlabel_resNetFeature, target_size, epsilon=1e-1, seed=101)

# check noisy
print(n_noise_data)
inspect_points = np.linspace(100, target_size, num=10).astype(int)
noise_cnt = []
for i in inspect_points:
  cnt = sum(x < n_noise_data for x in deepset_selected_rank[:i])
  noise_cnt.append(cnt)

print(inspect_points)
print(noise_cnt)

"""### Baselines"""

from torch.utils.data import Dataset, DataLoader, TensorDataset


class torchLogisticRegression(torch.nn.Module):
    def __init__(self, input_dim, output_dim):
        super(torchLogisticRegression, self).__init__()
        self.flatten = Flatten()
        self.linear = torch.nn.Linear(input_dim, output_dim)
        torch.nn.init.xavier_uniform(self.linear.weight)
        torch.nn.init.zeros_(self.linear.bias)
        self.input_dim = input_dim

    def forward(self, x, last=False):
        outputs = self.linear(x)
        if last:
            return outputs, x
        else:
            return outputs

    def get_embedding_dim(self):
        return self.input_dim

def torch_encoder_logistic_data_to_net(x_train, y_train, x_test, y_test, verbose=0):

  criterion = torch.nn.CrossEntropyLoss()
  net = torchLogisticRegression(x_train.shape[1], 10)
  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-7)

  tensor_x, tensor_y = torch.Tensor(x_train), torch.Tensor(y_train)
  fewshot_dataset = TensorDataset(tensor_x,tensor_y)
  train_loader = DataLoader(dataset=fewshot_dataset, batch_size=32, shuffle=True)
  tensor_x_test, tensor_y_test = torch.Tensor(x_test), torch.Tensor(y_test)
  test_dataset = TensorDataset(tensor_x_test,tensor_y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  for epoch in range(5):
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images)
          labels = Variable(labels.long())
          labels = torch.argmax(labels, dim=1)
          optimizer.zero_grad()
          outputs = net(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      for images, labels in test_loader:
          labels = labels.long()
          images = Variable(images)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          labels = torch.argmax(labels, dim=1)
          correct += (predicted == labels).sum()
      accuracy = 100 * correct/total
      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), accuracy))
  print('Model Accuracy: {}'.format(accuracy))
  return net

from distil.utils.data_handler import DataHandler_Points
from distil.active_learning_strategies import glister, FASS, BADGE

print(x_train_few.shape)
print(stl_x_unlabel_few.shape)
print(stl_x_unlabel_few_re.shape)
print(y_train_few.shape)
print(y_val_few.shape)
print(y_val_few_re.shape)

print(np_utils.to_categorical(y_train_few, 10).shape)
print(x_train_few_resNetFeature.shape)

'''
net = torch_encoder_logistic_data_to_net(x_train_few_resNetFeature, np_utils.to_categorical(y_train_few, 10), x_val_few_resNetFeature, np_utils.to_categorical(y_val_few, 10))


strategy_args = {'batch_size':target_size, 'submod':'facility_location', 'selection_type':'PerClass'}
strategy = FASS(x_train_few_resNetFeature, np_utils.to_categorical(y_train_few, 10), stl_x_unlabel_resNetFeature, net.cuda(), DataHandler_Points, 10, strategy_args)
fass_selected_rank = strategy.select(target_size)

strategy_args = {'batch_size' : target_size}
strategy = BADGE(x_train_few_resNetFeature, np_utils.to_categorical(y_train_few, 10), stl_x_unlabel_resNetFeature, net.cuda(), DataHandler_Points, 10, strategy_args)
badge_selected_rank = strategy.select(target_size)

strategy_args = {'batch_size' : target_size, 'lr':float(0.001)}
strategy = glister.GLISTER(x_train_few_resNetFeature, y_train_few, stl_x_unlabel_resNetFeature, net.cuda(), DataHandler_Points, 10, strategy_args, 
                           valid=True, X_val=x_val_few_resNetFeature, Y_val=y_val_few)
glister_selected_rank = strategy.select(target_size)
'''

net = torch_cifar_logistic_data_to_net(x_train_few, y_train_few, x_val_few, y_val_few)

strategy_args = {'batch_size':target_size, 'submod':'facility_location', 'selection_type':'PerClass'}
strategy = FASS(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args)
fass_selected_rank = strategy.select(target_size)

strategy_args = {'batch_size' : target_size}
strategy = BADGE(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args)
badge_selected_rank = strategy.select(target_size)

strategy_args = {'batch_size' : target_size, 'lr':float(0.001)}
strategy = glister.GLISTER(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args, 
                           valid=True, X_val=x_val_few, Y_val=y_val_few)
glister_selected_rank = strategy.select(target_size)

"""### Optimal"""

stl_x_train_few_cnnFeature , stl_x_unlabel_cnnFeature = pickle.load(open(prefix+'/active-learning-transfer/models/stl_optimal/STL-ResNet.feature', 'rb') )

input_dim = stl_x_unlabel_cnnFeature.shape[1] # 最多2000
print("input dim:", input_dim)
set_features=128
hidden_ext=128
hidden_reg=128
n_epoch=20
batch_size=32
lr = 1e-5

model_optimal = Utility_deepset(model=DeepSet(input_dim, set_features, hidden_ext, hidden_reg))

model_optimal.model.load_state_dict(torch.load(prefix + 'active-learning-transfer/models/stl_optimal' + '/stl_optimal_128_128_128_20_26.state_dict'))

"""#### optimal select rank"""

print(stl_x_unlabel_resNetFeature.shape)

_, optimal_dulo_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_optimal.model, stl_x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=101)

print(sum(np.array(optimal_dulo_selected_rank) < n_noise_data))

random_perm = np.random.permutation(range(unlabel_size))

"""### Save Ranks"""

rank_dict = {}

rank_dict['dulo'] = deepset_selected_rank
rank_dict['random'] = random_perm
rank_dict['optimal'] = optimal_dulo_selected_rank
rank_dict['fass'] = fass_selected_rank
rank_dict['badge'] = badge_selected_rank
rank_dict['glister'] = glister_selected_rank

print(model_dir)

pickle.dump(rank_dict, open(model_dir+"/cifar2stl_rank_dict.pkl","wb"))

"""## Run"""

print(stl_x_unlabel_few.shape)
print(stl_y_unlabel_few.shape)
print(stl_x_unlabel_few_1d.shape)

acc_greedy = []
acc_random = []
acc_optimal = []

inspect_points = np.linspace(1, target_size, num=10).astype(int)

random_perm = rank_dict['random']

for i in inspect_points:
  print(i)
  acc_greedy.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[deepset_selected_rank[:i]], stl_y_unlabel_few[deepset_selected_rank[:i]], stl_x_test, stl_y_test))
  acc_random.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[random_perm[:i]], stl_y_unlabel_few[random_perm[:i]], stl_x_test, stl_y_test))
  acc_optimal.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[optimal_dulo_selected_rank[:i]], stl_y_unlabel_few[optimal_dulo_selected_rank[:i]], stl_x_test, stl_y_test) )

acc_fass = []
acc_badge = []
acc_glister = []

random_perm = rank_dict['random']

for i in inspect_points:
  print(i)
  acc_fass.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[fass_selected_rank[:i]], stl_y_unlabel_few[fass_selected_rank[:i]], stl_x_test, stl_y_test))
  acc_badge.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[badge_selected_rank[:i]], stl_y_unlabel_few[badge_selected_rank[:i]], stl_x_test, stl_y_test))
  acc_glister.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[glister_selected_rank[:i]], stl_y_unlabel_few[glister_selected_rank[:i]], stl_x_test, stl_y_test))

plt.figure()

plt.plot(inspect_points, acc_greedy, label='DULO')
plt.plot(inspect_points, acc_random, label='Random')
plt.plot(inspect_points, acc_optimal, label='Optimal DULO')
plt.plot(inspect_points, acc_fass, label='FASS')
plt.plot(inspect_points, acc_badge, label='BADGE')
plt.plot(inspect_points, acc_glister, label='Glister')

plt.title('Data Selection, SmallCNN')
plt.ylabel('Classifier Accuracy')
plt.xlabel('# Selected Data Points')
plt.legend()
plt.show()

"""## Run (finetune)"""

def get_da_acc(net, x_test, y_test):
  
  net.eval()

  test_dataset = TensorDataset(x_test,y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  correct = 0
  total = 0
  for images, labels in test_loader:
      images = Variable(images.cuda(), requires_grad=False)
      labels = Variable(labels.cuda(), requires_grad=False)
      score = net(images)
      pred = score.data.max(1)[1] # get the index of the max log-probability
      correct += pred.eq(labels.data).cpu().sum()
      total += labels.size(0)
      
  accuracy = 100.0 * correct/total
  # 

  return accuracy / 100
  

model_dir = prefix + '/active-learning-transfer/models/cycada/cifar_to_stl_uda'
load_name = model_dir + "/ResNet_noise0.2_width2.pth"

depth = 26
width = 2 #16

net = ResNetCifar(depth, width, classes=10, channels=3).cuda()
net.load_state_dict(torch.load(load_name))

da_acc = get_da_acc(net, stl_x_test_re, stl_y_test_re)
print('DA Model Accuracy: {}'.format(da_acc))

def FeatExtractCPU(net, x):
  with torch.no_grad():
    feat = extractor_from_layer3(net)(x.cuda())
    # out = ext(x.float().cuda())
    feat = feat.view(feat.size(0), -1)
  return feat

# def FeatExtractCPU(net, x):
#   feat = extractor_from_layer3(net.cpu())(x.cpu())
#   feat = feat.view(feat.size(0), -1)
#   net.cuda()
#   return feat.cuda()

def setTrainFalse(layer):
  for i in range(4):
    layer[i].conv1.weight.trainable = False
    layer[i].conv2.weight.trainable = False



from scipy.stats import wasserstein_distance
def get_label(centroids, feat):
  best_dist = 1e9
  for k in range(len(centroids)):
    # import pdb;pdb.set_trace()
    dist = wasserstein_distance(centroids[k].detach().cpu(), feat.detach().cpu())
    if dist < best_dist:
      best_dist = dist 
      pred_label = k 
  
  return pred_label


def data_to_acc_finetune(adda_net_file, x_train, y_train, x_test, y_test, x_unlabel, verbose=0, lr=2e-6):
  net = ResNetCifar(26, 2, classes=10, channels=3).cuda()
  net.load_state_dict(torch.load(adda_net_file))
  net.train()


  net.conv1.weight.trainable = False
  setTrainFalse(net.layer1)
  setTrainFalse(net.layer2)
  setTrainFalse(net.layer3)
  

  optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0)
  criterion = nn.CrossEntropyLoss()

  # get centriods for K classes
  masks = []
  centroids = []
  for k in range(10):
    mask = np.zeros_like(y_train, dtype=bool)
    for i in range(len(y_train)):
      mask[i] = True if y_train[i] == k else False 
    
    cluster_x = x_train[mask]
    if cluster_x.shape[0] > 1:
      centroid = FeatExtractCPU(net, cluster_x)
      centroid = torch.mean(centroid, dim=0)
    else:
      print('Unnormal Alert!')
      centroid = cluster_syn[k]
    
    masks.append(mask)
    centroids.append(centroid)
  
  # predict fake labels for unlabel data
  y_unlabel = []
  # x_unlabel_feat = extractor_from_layer3(net)(x_unlabel.cuda())
  # x_unlabel_feat = x_unlabel_feat.view(x_unlabel_feat.size(0), -1)
  x_unlabel_feat = FeatExtractCPU(net, x_unlabel)
  for i in range(x_unlabel.size(0)):
    y_unlabel.append(get_label(centroids, x_unlabel_feat[i]))

  y_unlabel = torch.tensor(y_unlabel)

  x_train_new = torch.cat((x_train, x_unlabel), dim=0)
  y_train_new = torch.cat((y_train, y_unlabel), dim=0)

  finetune_dataset = TensorDataset(x_train_new,y_train_new)
  train_loader = DataLoader(dataset=finetune_dataset, batch_size=32, shuffle=True)
  test_dataset = TensorDataset(x_test,y_test)
  test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

  best_acc = 0
  for epoch in range(35):
      net.train()
      for i, (images, labels) in enumerate(train_loader):
          images = Variable(images.cuda(), requires_grad=False)
          labels = Variable(labels.cuda(), requires_grad=False)
          optimizer.zero_grad()
          # import pdb;pdb.set_trace()
          score = net(images)
          loss = criterion(score, labels)
          loss.backward()
          optimizer.step()
      correct = 0
      total = 0
      test_acc = get_da_acc(net, x_test, y_test)

      if verbose:
        print("Epoch: {}. Loss: {}. Accuracy: {}.".format(epoch, loss.item(), test_acc*100.0))

      if test_acc > best_acc:
        best_acc = test_acc
      else:
        break
  print('Model Accuracy: {}'.format(best_acc*100.0))


  return best_acc


def get_unlabel(selected_rank):
  idx_unlabel = np.ones_like(stl_y_unlabel_few_re, dtype=bool)
  idx_unlabel[selected_rank] = False
  return stl_x_unlabel_few_re[idx_unlabel]

adda_file = model_dir + "/ResNet_noise0.2_width2.pth"

acc_greedy_ft = [da_acc]
acc_random_ft = [da_acc]
acc_optimal_ft = [da_acc]
acc_ds = [da_acc]

inspect_points = np.linspace(100, 1000, num=10).astype(int)

# random_perm = rank_dict['random']

# stl_x_unlabel_few, stl_y_unlabel_few

for i in inspect_points:
  acc_ds.append(da_acc)
  acc_greedy_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[deepset_selected_rank[:i]], stl_y_unlabel_few_re[deepset_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(deepset_selected_rank[:i])))
  # import pdb;pdb.set_trace()

  acc_random_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[random_perm[:i]], stl_y_unlabel_few_re[random_perm[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(random_perm[:i])))

  acc_optimal_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[optimal_dulo_selected_rank[:i]], stl_y_unlabel_few_re[optimal_dulo_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(optimal_dulo_selected_rank[:i])))

acc_fass_ft = [da_acc]
acc_badge_ft = [da_acc]
acc_glister_ft = [da_acc]


for i in inspect_points:
  idx_unlabel = np.ones_like(usps_y_unlabel_re, dtype=bool)
  idx_unlabel[fass_selected_rank[:i]] = False
  acc_fass_ft.append(data_to_acc_finetune_new(adda_file, usps_x_unlabel_re[fass_selected_rank[:i]], usps_y_unlabel_re[fass_selected_rank[:i]], usps_x_test_re, usps_y_test_re, usps_x_unlabel_re[idx_unlabel], lr=2e-6))
  
  idx_unlabel = np.ones_like(usps_y_unlabel_re, dtype=bool)
  idx_unlabel[badge_selected_rank[:i]] = False
  acc_badge_ft.append(data_to_acc_finetune_new(adda_file, usps_x_unlabel_re[badge_selected_rank[:i]], usps_y_unlabel_re[badge_selected_rank[:i]], usps_x_test_re, usps_y_test_re, usps_x_unlabel_re[idx_unlabel], lr=2e-6))
  
  idx_unlabel = np.ones_like(usps_y_unlabel_re, dtype=bool)
  idx_unlabel[glister_selected_rank[:i]] = False
  acc_glister_ft.append(data_to_acc_finetune_new(adda_file, usps_x_unlabel_re[glister_selected_rank[:i]], usps_y_unlabel_re[glister_selected_rank[:i]], usps_x_test_re, usps_y_test_re, usps_x_unlabel_re[idx_unlabel], lr=2e-6))

"""### Plot"""

# check noisy
inspect_points = np.linspace(100, target_size, num=10).astype(int)
noise_cnt = []
for i in inspect_points:
  cnt = sum(x < 1700 for x in deepset_selected_rank[:i])
  noise_cnt.append(cnt)

print(inspect_points)
print(noise_cnt)

inspect_points = np.linspace(0, target_size, num=11).astype(int)
plt.figure()
plt.plot(inspect_points, acc_greedy_ft, label='DULO')
plt.plot(inspect_points, acc_random_ft, label='Random')
plt.plot(inspect_points, acc_optimal_ft, label='Optimal DULO')
plt.plot(inspect_points, acc_fass_ft, label='FASS')
plt.plot(inspect_points, acc_badge_ft, label='BADGE')
plt.plot(inspect_points, acc_glister_ft, label='Glister')
plt.plot(inspect_points, acc_ds, label='Domain Adaptation')

plt.title('Data Selection (Finetune)')
plt.ylabel('Classifier Accuracy')
plt.xlabel('# Selected Data Points')
plt.legend()
plt.show()

"""## Multi seed"""

acc_dulo_dict = {}
acc_random_dict = {}
acc_optimal_dict = {}
acc_fass_dict = {}
acc_badge_dict = {}
acc_glister_dict = {}

rank_dulo_dict = {}
rank_random_dict = {}
rank_optimal_dict = {}
rank_fass_dict = {}
rank_badge_dict = {}
rank_glister_dict = {}

net = torch_cifar_logistic_data_to_net(x_train_few, y_train_few, x_val_few, y_val_few)

for seed in range(12, 18):
  _, deepset_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model.model, stl_x_unlabel_resNetFeature, target_size, epsilon=1e-4, seed=seed)
  random_perm = np.random.permutation(range(unlabel_size))
  _, optimal_dulo_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_optimal.model, stl_x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=seed)

  
  strategy_args = {'batch_size':target_size, 'submod':'facility_location', 'selection_type':'PerClass'}
  strategy = FASS(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args)
  fass_selected_rank = strategy.select(target_size)

  strategy_args = {'batch_size' : target_size}
  strategy = BADGE(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args)
  badge_selected_rank = strategy.select(target_size)

  strategy_args = {'batch_size' : target_size, 'lr':float(0.001)}
  strategy = glister.GLISTER(x_train_few, y_train_few, stl_x_unlabel_few_re.numpy(), net.cuda(), DataHandler_Points, 10, strategy_args, 
                            valid=True, X_val=x_val_few, Y_val=y_val_few)
  glister_selected_rank = strategy.select(target_size)



  rank_dulo_dict[seed] = deepset_selected_rank
  rank_random_dict[seed] = random_perm
  rank_optimal_dict[seed] = optimal_dulo_selected_rank
  rank_fass_dict[seed] = fass_selected_rank
  rank_badge_dict[seed] = badge_selected_rank
  rank_glister_dict[seed] = glister_selected_rank


  acc_greedy = []
  acc_random = []
  acc_optimal = []
  acc_fass = []
  acc_badge = []
  acc_glister = []

  inspect_points = np.linspace(1, target_size, num=10).astype(int)

  for i in inspect_points:
    acc_greedy.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[deepset_selected_rank[:i]], stl_y_unlabel_few[deepset_selected_rank[:i]], stl_x_test, stl_y_test))
    acc_random.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[random_perm[:i]], stl_y_unlabel_few[random_perm[:i]], stl_x_test, stl_y_test))
    acc_optimal.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[optimal_dulo_selected_rank[:i]], stl_y_unlabel_few[optimal_dulo_selected_rank[:i]], stl_x_test, stl_y_test) )

    acc_fass.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[fass_selected_rank[:i]], stl_y_unlabel_few[fass_selected_rank[:i]], stl_x_test, stl_y_test))
    acc_badge.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[badge_selected_rank[:i]], stl_y_unlabel_few[badge_selected_rank[:i]], stl_x_test, stl_y_test))
    acc_glister.append(torch_stl_smallcnn_data_to_acc(stl_x_unlabel_few[glister_selected_rank[:i]], stl_y_unlabel_few[glister_selected_rank[:i]], stl_x_test, stl_y_test))
    

  acc_dulo_dict[seed] = acc_greedy
  acc_random_dict[seed] = acc_random
  acc_optimal_dict[seed] = acc_optimal
  acc_fass_dict[seed] = acc_fass
  acc_badge_dict[seed] = acc_badge
  acc_glister_dict[seed] = acc_glister

"""### load rank and acc"""

corruption = 'Noisy'
model_dir = prefix + '/active-learning-transfer/models/cycada/cifar_to_stl_uda'

rank_dulo_dict, rank_random_dict, rank_optimal_dict, rank_fass_dict, rank_badge_dict, rank_glister_dict = pickle.load(open(model_dir+"/rank_12_18_{}.pkl".format(corruption),"rb"))
acc_dulo_dict, acc_random_dict, acc_optimal_dict, acc_fass_dict, acc_badge_dict, acc_glister_dict = pickle.load(open(model_dir+"/acc_12_18_{}.pkl".format(corruption),"rb"))

# check noisy
inspect_points = np.linspace(100, target_size, num=10).astype(int)
noise_cnt = []
for i in inspect_points:
  cnt = sum(x < n_noise_data for x in rank_glister_dict[12][:i])
  noise_cnt.append(cnt)

print(inspect_points)
print(noise_cnt)

"""### plot"""

target_size = 1000
corruption = 'Noisy'

inspect_points = np.linspace(100, target_size, num=10).astype(int)

acc_dulo = np.zeros((6, 10))
acc_random = np.zeros((6, 10))
acc_optimal = np.zeros((6, 10))
acc_fass = np.zeros((6, 10))
acc_badge = np.zeros((6, 10))
acc_glister = np.zeros((6, 10))

for i in range(6):
  seed = i+12
  acc_dulo[i] = acc_dulo_dict[seed]
  acc_random[i] = acc_random_dict[seed]
  acc_optimal[i] = acc_optimal_dict[seed]
  acc_fass[i] = acc_fass_dict[seed]
  acc_badge[i] = acc_badge_dict[seed]
  acc_glister[i] = acc_glister_dict[seed]

plt.figure()

def plot_fill(record, name, col):
  if name in ['DULO', 'Optimal']:
    plt.plot(inspect_points, np.mean(record, axis=0), label=name, linewidth=2, color=col)
  else:
    plt.plot(inspect_points, np.mean(record, axis=0), label=name, linewidth=1, color=col)
  plt.fill_between(inspect_points, np.mean(record, axis=0)+np.std(record, axis=0), np.mean(record, axis=0)-np.std(record, axis=0), alpha=0.2, color=col)

plot_fill(acc_dulo, 'DULO', 'C0')
plot_fill(acc_random, 'Random', 'C1')
plot_fill(acc_optimal, 'Optimal', 'C2')
plot_fill(acc_fass, 'FASS', 'C3')
plot_fill(acc_badge, 'BADGE', 'C4')
plot_fill(acc_glister, 'GLISTER', 'C5')

plt.title('CIFAR10->STL10, SmallCNN, ' + corruption) 
plt.ylabel('Classifier Accuracy')
plt.xlabel('# Selected Data Points')
plt.legend()
plt.show()

print(corruption)

model_dir = prefix + '/active-learning-transfer/models/cycada/cifar_to_stl_uda'

pickle.dump([rank_dulo_dict, rank_random_dict, rank_optimal_dict, rank_fass_dict, rank_badge_dict, rank_glister_dict], open(model_dir+"/rank_12_18_{}_1.pkl".format(corruption),"wb"))
pickle.dump([acc_dulo_dict, acc_random_dict, acc_optimal_dict, acc_fass_dict, acc_badge_dict, acc_glister_dict], open(model_dir+"/acc_12_18_{}_1.pkl".format(corruption),"wb"))

print(model_dir)



"""## Multi seed (fine-tune)"""

print(rank_dulo_dict[12])
print(rank_random_dict[12])
print(rank_optimal_dict[12])
print(rank_fass_dict[12])
print(rank_badge_dict[12])
print(rank_glister_dict[12])

def check_selection_unnormal(set_selected):
  # input is torch tensor
  for i in range(10):
    if i in set_selected:
      continue
    else:
      return True
  return False


def get_unlabel(selected_rank):
  idx_unlabel = np.ones_like(stl_y_unlabel_few_re, dtype=bool)
  idx_unlabel[selected_rank] = False
  return stl_x_unlabel_few_re[idx_unlabel]

corruption = 'Noisy'


acc_dulo_dict_ft = {}
acc_random_dict_ft = {}
acc_optimal_dict_ft = {}
acc_fass_dict_ft = {}
acc_badge_dict_ft = {}
acc_glister_dict_ft = {}

rank_dulo_dict, rank_random_dict, rank_optimal_dict, rank_fass_dict, rank_badge_dict, rank_glister_dict = pickle.load(open(model_dir+"/rank_12_18_{}.pkl".format(corruption),"rb"))

adda_file = model_dir + "/ResNet_noise0.2_width2.pth"

for seed in range(12, 18):

  deepset_selected_rank = rank_dulo_dict[seed]
  random_perm = rank_random_dict[seed]
  optimal_dulo_selected_rank = rank_optimal_dict[seed]
  fass_selected_rank = rank_fass_dict[seed]
  badge_selected_rank = rank_badge_dict[seed] 
  glister_selected_rank = rank_glister_dict[seed]

  
  acc_greedy_ft = [da_acc]
  acc_random_ft = [da_acc]
  acc_optimal_ft = [da_acc]
  acc_fass_ft  = [da_acc]
  acc_badge_ft = [da_acc]
  acc_glister_ft = [da_acc]

  inspect_points = np.linspace(100, 1000, num=10).astype(int)

  for i in inspect_points:
    print(i)

    acc_greedy_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[deepset_selected_rank[:i]], stl_y_unlabel_few_re[deepset_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(deepset_selected_rank[:i])[:i]))
    acc_random_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[random_perm[:i]], stl_y_unlabel_few_re[random_perm[:i]], stl_x_test_re, stl_y_test_re,get_unlabel(random_perm[:i])[:i]))
    acc_optimal_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[optimal_dulo_selected_rank[:i]], stl_y_unlabel_few_re[optimal_dulo_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(optimal_dulo_selected_rank[:i])[:i]))
    
    
    acc_fass_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[fass_selected_rank[:i]], stl_y_unlabel_few_re[fass_selected_rank[:i]], stl_x_test_re, stl_y_test_re,get_unlabel(fass_selected_rank[:i])[:i]))
    acc_badge_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[badge_selected_rank[:i]], stl_y_unlabel_few_re[badge_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(badge_selected_rank[:i])[:i]))
    acc_glister_ft.append(data_to_acc_finetune(adda_file, stl_x_unlabel_few_re[glister_selected_rank[:i]], stl_y_unlabel_few_re[glister_selected_rank[:i]], stl_x_test_re, stl_y_test_re, get_unlabel(glister_selected_rank[:i])[:i]))


  acc_dulo_dict_ft[seed] = acc_greedy_ft
  acc_random_dict_ft[seed] = acc_random_ft
  acc_optimal_dict_ft[seed] = acc_optimal_ft
  acc_fass_dict_ft[seed] = acc_fass_ft
  acc_badge_dict_ft[seed] = acc_badge_ft
  acc_glister_dict_ft[seed] = acc_glister_ft

"""### load rank and acc"""

corruption = 'Noisy'
model_dir = prefix + '/active-learning-transfer/models/cycada/cifar_to_stl_uda'

rank_dulo_dict, rank_random_dict, rank_optimal_dict, rank_fass_dict, rank_badge_dict, rank_glister_dict = pickle.load(open(model_dir+"/rank_12_18_{}.pkl".format(corruption),"rb"))
acc_dulo_dict_ft, acc_random_dict_ft, acc_optimal_dict_ft, acc_fass_dict_ft, acc_badge_dict_ft, acc_glister_dict_ft = pickle.load(open(model_dir+"/accFineTune_12_18_{}.pkl".format(corruption),"rb"))

"""### Plot"""

target_size = 1000
inspect_points = np.linspace(0, target_size, num=11).astype(int)


acc_dulo = np.zeros((6, 11))
acc_random = np.zeros((6, 11))
acc_optimal = np.zeros((6, 11))
acc_fass = np.zeros((6, 11))
acc_badge = np.zeros((6, 11))
acc_glister = np.zeros((6, 11))

for i in range(6):
  seed = i+12
  acc_dulo[i] = acc_dulo_dict_ft[seed]
  acc_random[i] = acc_random_dict_ft[seed]
  acc_optimal[i] = acc_optimal_dict_ft[seed]
  acc_fass[i] = acc_fass_dict_ft[seed]
  acc_badge[i] = acc_badge_dict_ft[seed]
  acc_glister[i] = acc_glister_dict_ft[seed]

plt.figure()

def plot_fill(record, name, col):
  plt.plot(inspect_points, np.mean(record, axis=0), label=name, color=col)
  plt.fill_between(inspect_points, np.mean(record, axis=0)+np.std(record, axis=0), np.mean(record, axis=0)-np.std(record, axis=0), alpha=0.2, color=col)

plot_fill(acc_dulo, 'DULO', 'C0')
plot_fill(acc_random, 'Random', 'C1')
plot_fill(acc_optimal, 'Optimal', 'C2')
plot_fill(acc_fass, 'FASS', 'C3')
plot_fill(acc_badge, 'BADGE', 'C4')
plot_fill(acc_glister, 'GLISTER', 'C5')

plt.title('CIFAR10->STL10, Finetune, ' + corruption)
plt.ylabel('Classifier Accuracy')
plt.xlabel('# Selected Data Points')
plt.legend()
plt.show()

print(corruption)

pickle.dump([acc_dulo_dict_ft, acc_random_dict_ft, acc_optimal_dict_ft, acc_fass_dict_ft, acc_badge_dict_ft, acc_glister_dict_ft], open(model_dir+"/accFineTune_12_18_{}m1.pkl".format(corruption),"wb"))

print(model_dir)