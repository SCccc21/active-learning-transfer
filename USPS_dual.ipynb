{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G_-MNjO8aLS"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m9sihl5nxMz8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Flatten, Dense, LeakyReLU, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.models import clone_model\n",
    "from keras import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.optimize as op \n",
    "import seaborn as sns\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dwSu_55-uKM",
    "outputId": "62aed8dd-8ce2-4bf8-fb6a-4aadb2f97b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "prefix = '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2bDzlaSEs_n"
   },
   "source": [
    "### Sample Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wjGtjvQpQVWn"
   },
   "outputs": [],
   "source": [
    "# utility_func_args = [x_train, y_train, x_val, y_val]\n",
    "def sample_utility(n, size_min, size_max, utility_func, utility_func_args, random_state, ub_prob=0.2, verbose=False):\n",
    "\n",
    "  x_train, y_train, x_val, y_val = utility_func_args\n",
    "\n",
    "  X_feature_test = []\n",
    "  y_feature_test = []\n",
    "\n",
    "  x_train = np.array(x_train)\n",
    "  y_train = np.array(y_train)\n",
    "\n",
    "  N = len(y_train)\n",
    "\n",
    "  np.random.seed(random_state)\n",
    "  \n",
    "  for i in range(n):\n",
    "    if verbose:\n",
    "      print('{} / {}'.format(i, n))\n",
    "\n",
    "    n_select = np.random.choice(range(size_min, size_max))\n",
    "\n",
    "    subset_index = []\n",
    "\n",
    "    \"\"\"\n",
    "    if unbalance:\n",
    "      n_per_class = int(N / 10)\n",
    "      alpha = np.ones(10)\n",
    "      alpha[np.random.choice(range(10))] = np.random.choice(range(1, 50))\n",
    "    else:\n",
    "      alpha = np.random.choice(range(1, 20), size=10, replace=True)\n",
    "    \"\"\"\n",
    "\n",
    "    toss = np.random.uniform()\n",
    "\n",
    "    # With probability ub_prob, sample a class-imbalanced subset\n",
    "    if toss > 1-ub_prob:\n",
    "      n_per_class = int(N / 10)\n",
    "      alpha = np.ones(10)\n",
    "      alpha[np.random.choice(range(10))] = np.random.choice(range(1, 50))\n",
    "    else:\n",
    "      alpha = np.random.choice(range(90, 100), size=10, replace=True)\n",
    "\n",
    "    p = np.random.dirichlet(alpha=alpha)\n",
    "    occur = np.random.choice(range(10), size=n_select, replace=True, p=p)\n",
    "    counts = np.array([np.sum(occur==i) for i in range(10)])\n",
    "    \n",
    "    for i in range(10):\n",
    "      ind_i = np.where(np.argmax(y_train, 1)==i)[0]\n",
    "      if len(ind_i) > counts[i]:\n",
    "        selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=False)\n",
    "      else:\n",
    "        selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=True)\n",
    "      subset_index = subset_index + list(selected_ind_i)\n",
    "\n",
    "    subset_index = np.array(subset_index)\n",
    "\n",
    "    y_feature_test.append(utility_func(x_train[subset_index], y_train[subset_index], x_val, y_val))\n",
    "    X_feature_test.append( subset_index )\n",
    "\n",
    "  return X_feature_test, y_feature_test\n",
    "\n",
    "\n",
    "def sample_utility_veryub(n, size_min, size_max, utility_func, utility_func_args, random_state, ub_prob=0.2, verbose=False):\n",
    "\n",
    "  x_train, y_train, x_val, y_val = utility_func_args\n",
    "\n",
    "  X_feature_test = []\n",
    "  y_feature_test = []\n",
    "\n",
    "  x_train = np.array(x_train)\n",
    "  y_train = np.array(y_train)\n",
    "\n",
    "  N = len(y_train)\n",
    "\n",
    "  np.random.seed(random_state)\n",
    "  \n",
    "  for i in range(n):\n",
    "    if verbose:\n",
    "      print('{} / {}'.format(i, n))\n",
    "\n",
    "    n_select = np.random.choice(range(size_min, size_max))\n",
    "\n",
    "    if n_select > 0:\n",
    "      subset_index = []\n",
    "\n",
    "      toss = np.random.uniform()\n",
    "\n",
    "      # With probability ub_prob, sample a class-imbalanced subset\n",
    "      if toss > 1-ub_prob:\n",
    "        alpha = np.random.choice(range(1, 100), size=10, replace=True)\n",
    "      else:\n",
    "        alpha = np.random.choice(range(90, 100), size=10, replace=True)\n",
    "\n",
    "      p = np.random.dirichlet(alpha=alpha)\n",
    "      occur = np.random.choice(range(10), size=n_select, replace=True, p=p)\n",
    "      counts = np.array([np.sum(occur==i) for i in range(10)])\n",
    "      \n",
    "      for i in range(10):\n",
    "        ind_i = np.where(np.argmax(y_train, 1)==i)[0]\n",
    "        if len(ind_i) > counts[i]:\n",
    "          selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=False)\n",
    "        else:\n",
    "          selected_ind_i = np.random.choice(ind_i, size=counts[i], replace=True)\n",
    "        subset_index = subset_index + list(selected_ind_i)\n",
    "\n",
    "      subset_index = np.array(subset_index)\n",
    "\n",
    "      y_feature_test.append(utility_func(x_train[subset_index], y_train[subset_index], x_val, y_val))\n",
    "      X_feature_test.append( subset_index )\n",
    "    else:\n",
    "      y_feature_test.append(0.1)\n",
    "      X_feature_test.append( np.array([]) )\n",
    "\n",
    "  return X_feature_test, y_feature_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GAMKeuA9kaA"
   },
   "source": [
    "### Deep Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RTtXlOWs9kaB"
   },
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, set_features=128, hidden_ext=128, hidden_reg=128):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = set_features\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_ext, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_ext, hidden_ext, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_ext, set_features, bias=False)\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(set_features, hidden_reg, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_reg, hidden_reg, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_reg, int(hidden_reg/2), bias=False),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(int(hidden_reg/2), 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.add_module('0', self.feature_extractor)\n",
    "        self.add_module('1', self.regressor)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for module in self.children():\n",
    "            reset_op = getattr(module, \"reset_parameters\", None)\n",
    "            if callable(reset_op):\n",
    "                reset_op()\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.sum(dim=1)\n",
    "        x = self.regressor(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'Feature Exctractor=' + str(self.feature_extractor) \\\n",
    "            + '\\n Set Feature' + str(self.regressor) + ')'\n",
    "\n",
    "\n",
    "class DeepSet_cifar(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, set_features=512):\n",
    "        super(DeepSet_cifar, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = set_features\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, set_features)\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(set_features, 512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.add_module('0', self.feature_extractor)\n",
    "        self.add_module('1', self.regressor)\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for module in self.children():\n",
    "            reset_op = getattr(module, \"reset_parameters\", None)\n",
    "            if callable(reset_op):\n",
    "                reset_op()\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.sum(dim=1)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'Feature Exctractor=' + str(self.feature_extractor) \\\n",
    "            + '\\n Set Feature' + str(self.regressor) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kR9g5fHmdplj"
   },
   "outputs": [],
   "source": [
    "class DeepSet_dual(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, set_features=128, hidden_ext=128, hidden_reg=128):\n",
    "        super(DeepSet_dual, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = set_features\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_ext, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_ext, hidden_ext, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_ext, set_features, bias=False)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(set_features, hidden_reg, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_reg, hidden_reg, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_reg, int(hidden_reg/2), bias=False),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(int(hidden_reg/2), 3) #NOTE: n_classes=3 as 0 <, 1 =, 2 >\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.add_module('0', self.feature_extractor)\n",
    "        self.add_module('1', self.classifier)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for module in self.children():\n",
    "            reset_op = getattr(module, \"reset_parameters\", None)\n",
    "            if callable(reset_op):\n",
    "                reset_op()\n",
    "            \n",
    "    def forward(self, input1, input2):\n",
    "        x = torch.cat((input1, input2), dim=2)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.sum(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.linear(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'Feature Exctractor=' + str(self.feature_extractor) \\\n",
    "            + '\\n Set Feature' + str(self.classifier) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woCNVXrjaeIX"
   },
   "source": [
    "### Deepset Utility Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gyTePxkO6MN3"
   },
   "outputs": [],
   "source": [
    "class Utility_deepset_dual(object):\n",
    "\n",
    "    def __init__(self, model=None):\n",
    "\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "          self.model = DeepSet(in_dims).cuda()\n",
    "        else:\n",
    "          self.model = model.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.model.linear.bias = torch.nn.Parameter(torch.tensor([-2.1972]))\n",
    "        self.model.linear.bias.requires_grad = False\n",
    "        #print(self.model.linear.bias)\n",
    "        self.model.cuda()\n",
    "        #print(self.model.linear.bias)\n",
    "        \n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.l2 = nn.MSELoss(reduction='sum')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    # train_data: x_train_few\n",
    "    # train_set: (X_feature, y_feature)\n",
    "    # valid_set: (X_feature_test, y_feature_test)\n",
    "    def compare(self, y1, y2, tau=0.1):\n",
    "      if y1 > y2+tau:\n",
    "        return 2\n",
    "      elif y2+tau > y1 and y1 >= y2-tau:\n",
    "        return 1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    def fit(self, train_data, train_set, valid_set, n_epoch, batch_size=32, lr=1e-3, n_pair=10000):\n",
    "\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "        #scheduler = StepLR(self.optim, step_size=10, gamma=0.1)\n",
    "        scheduler = MultiStepLR(self.optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "        train_data = copy.deepcopy(train_data)\n",
    "        N = train_data.shape[0]\n",
    "        k = train_data.shape[1]\n",
    "\n",
    "        X_feature, y_feature = train_set\n",
    "        X_feature_test, y_feature_test = valid_set\n",
    "        train_size = len(y_feature)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "          # sample paired inputs\n",
    "          X_pair1, X_pair2, y_pair = [], [], []\n",
    "          for s in range(n_pair):\n",
    "            select = np.random.choice(train_size, size=2, replace=False)\n",
    "            X_pair1.append(X_feature[select[0]])\n",
    "            X_pair2.append(X_feature[select[1]])\n",
    "            y_pair.append( self.compare(y_feature[select[0]], y_feature[select[1]], tau=0.1) )\n",
    "\n",
    "          # train\n",
    "          train_loss = 0\n",
    "          for j in range(n_pair//batch_size):\n",
    "            start_ind = j*batch_size\n",
    "            batch_X1, batch_X2, batch_y = [], [], []\n",
    "            for i in range(start_ind, min(start_ind+batch_size, n_pair)):\n",
    "              b1, b2 = np.zeros((N, k)), np.zeros((N, k))\n",
    "              if len(X_pair1[i]) > 0 and len(X_pair2[i] > 0):\n",
    "                b1[:len(X_pair1[i])] = train_data[X_pair1[i]]\n",
    "                b2[:len(X_pair2[i])] = train_data[X_pair2[i]]\n",
    "\n",
    "              batch_X1.append( b1 )\n",
    "              batch_X2.append( b2)\n",
    "              batch_y.append( y_pair[i] )\n",
    "\n",
    "            # print('tot{} batch {} length1 {} length2 {}'.format(n_pair//batch_size, j, len(batch_X1), len(batch_X2)))\n",
    "            batch_X1 = np.stack(batch_X1)\n",
    "            batch_X2 = np.stack(batch_X2)\n",
    "            batch_X1, batch_X2, batch_y = torch.FloatTensor(batch_X1).cuda(), torch.FloatTensor(batch_X2).cuda(), torch.LongTensor(batch_y).cuda()\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            y_pred = self.model(batch_X1, batch_X2)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            loss = self.criterion(y_pred, batch_y)\n",
    "            loss_val = np.asscalar(loss.data.cpu().numpy())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            train_loss += loss_val\n",
    "          train_loss /= train_size\n",
    "          test_loss = self.evaluate(train_data, valid_set, test_size=200)\n",
    "          scheduler.step()\n",
    "          print('Epoch %s Train Loss %s Test Loss %s' % (epoch, train_loss, test_loss))\t\t\t\n",
    "    \n",
    "    def evaluate(self, train_data, valid_set, test_size=2000):\n",
    "\n",
    "        N, k = train_data.shape\n",
    "        X_feature_test, y_feature_test = valid_set\n",
    "\n",
    "        X_pair1, X_pair2, y_pair = [], [], []\n",
    "        for s in range(test_size):\n",
    "          select = np.random.choice(len(y_feature_test), size=2, replace=False)\n",
    "          X_pair1.append(X_feature_test[select[0]])\n",
    "          X_pair2.append(X_feature_test[select[1]])\n",
    "          y_pair.append( self.compare(y_feature_test[select[0]], y_feature_test[select[1]]) )\n",
    "\n",
    "\n",
    "        test_loss = 0\n",
    "        for i in range(test_size):\n",
    "            b1, b2 = np.zeros((N, k)), np.zeros((N, k))\n",
    "            if len(X_pair1[i]) > 0 and len(X_pair2[i]) > 0:\n",
    "              b1[:len(X_pair1[i])] = train_data[X_pair1[i]]\n",
    "              b2[:len(X_pair2[i])] = train_data[X_pair2[i]]\n",
    "\n",
    "            batch_X1, batch_X2, batch_y = torch.FloatTensor(b1).cuda(), torch.FloatTensor(b2).cuda(), torch.LongTensor(y_pair[i:i+1]).cuda()\n",
    "            batch_X1, batch_X2, batch_y = batch_X1.reshape((1, N, k)), batch_X2.reshape((1, N, k)), batch_y.reshape(-1)\n",
    "            y_pred = self.model(batch_X1, batch_X2)\n",
    "\n",
    "            loss = self.criterion(y_pred, batch_y)\n",
    "            loss_val = np.asscalar(loss.data.cpu().numpy())\n",
    "            test_loss += loss_val\n",
    "        test_loss /= test_size\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bzKwwkOHKYNE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "class findMostValuableSample_dual(object):\n",
    "  def __init__(self, model, unlabeled_set, target_size, n_sample=5000):\n",
    "\n",
    "      self.model = model\n",
    "      self.unlabeled_set = unlabeled_set\n",
    "      self.target_size = target_size\n",
    "      self.n_sample = n_sample\n",
    "\n",
    "  def compare(self, s1, s2):\n",
    "    assert len(s1) == len(s2)\n",
    "    # N = self.unlabeled_set.shape[0]\n",
    "    # k = self.n_sample\n",
    "    # b1, b2 = np.zeros((N, k)), np.zeros((N, k))\n",
    "    # b1[:len(s1)] = self.unlabled_set[s1]\n",
    "    # b2[:len(s2)] = self.unlabled_set[s2]\n",
    "    input_dim = self.unlabeled_set.shape[1]\n",
    "    x1 = torch.FloatTensor(self.unlabeled_set[s1].reshape((1, len(s1), input_dim))).cuda()\n",
    "    x2 = torch.FloatTensor(self.unlabeled_set[s2].reshape((1, len(s2), input_dim))).cuda()\n",
    "    y_pred = self.model(x1, x2)\n",
    "    return torch.argmax(y_pred, dim=1).view(-1)\n",
    "\n",
    "  def merge_sort(self, lst):\n",
    "    if len(lst) <= 1:\n",
    "      return lst\n",
    "    mid = len(lst) // 2\n",
    "    left = self.merge_sort(lst[:mid])\n",
    "    right = self.merge_sort(lst[mid:])\n",
    "    return self.merge(left, right)\n",
    "\n",
    "  def merge(self, left, right):\n",
    "    res = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "      if self.compare(left[i], right[j]) == 2:\n",
    "        res.append(left[i])\n",
    "      else:\n",
    "        res.append(right[i])\n",
    "    res += left[i:]\n",
    "    res += right[j:]\n",
    "    return res\n",
    "  \n",
    "  def get_rank(self):\n",
    "    start = time.time()\n",
    "    N = self.unlabeled_set.shape[0] #unlabel_size = 2000\n",
    "    X_s = []\n",
    "    for i in range(self.n_sample):\n",
    "      select = np.random.choice(N, self.target_size, replace=False)\n",
    "      X_s.append(select)\n",
    "    \n",
    "    rank = self.merge_sort(X_s)\n",
    "    print(\"Time: {:.2f}\".format(time.time()-start))\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nsWXqmyHaaI1"
   },
   "outputs": [],
   "source": [
    "class Utility_deepset(object):\n",
    "\n",
    "    def __init__(self, model=None):\n",
    "\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "          self.model = DeepSet(in_dims).cuda()\n",
    "        else:\n",
    "          self.model = model.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.model.linear.bias = torch.nn.Parameter(torch.tensor([-2.1972]))\n",
    "        self.model.linear.bias.requires_grad = False\n",
    "        #print(self.model.linear.bias)\n",
    "        self.model.cuda()\n",
    "        #print(self.model.linear.bias)\n",
    "        \n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.l2 = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    # train_data: x_train_few\n",
    "    # train_set: (X_feature, y_feature)\n",
    "    # valid_set: (X_feature_test, y_feature_test)\n",
    "    def fit(self, train_data, train_set, valid_set, n_epoch, batch_size=32, lr=1e-3):\n",
    "\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "        #scheduler = StepLR(self.optim, step_size=10, gamma=0.1)\n",
    "        scheduler = MultiStepLR(self.optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "        train_data = copy.deepcopy(train_data)\n",
    "        N = train_data.shape[0]\n",
    "        k = train_data.shape[1]\n",
    "\n",
    "        X_feature, y_feature = train_set\n",
    "        X_feature_test, y_feature_test = valid_set\n",
    "        train_size = len(y_feature)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "\n",
    "          # Shuffle training utility samples\n",
    "          ind = np.arange(train_size, dtype=int)\n",
    "          np.random.shuffle(ind)\n",
    "          X_feature = [X_feature[i] for i in ind]\n",
    "          y_feature = y_feature[ind]\n",
    "\n",
    "          train_loss = 0\n",
    "          start_ind = 0\n",
    "\n",
    "          for j in range(train_size//batch_size):\n",
    "            start_ind = j*batch_size\n",
    "            batch_X, batch_y = [], []\n",
    "            for i in range(start_ind, min(start_ind+batch_size, train_size)):\n",
    "\n",
    "              b = np.zeros((N, k))\n",
    "              if len(X_feature[i]) > 0:\n",
    "                selected_train_data = train_data[X_feature[i]]\n",
    "                b[:len(X_feature[i])] = selected_train_data\n",
    "\n",
    "              batch_X.append( b )\n",
    "              batch_y.append( [y_feature[i]] )\n",
    "\n",
    "            batch_X = np.stack(batch_X)\n",
    "            batch_X, batch_y = torch.FloatTensor(batch_X).cuda(), torch.FloatTensor(batch_y).cuda()\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            y_pred = self.model(batch_X)\n",
    "            loss = self.l2(y_pred, batch_y)\n",
    "            loss_val = np.asscalar(loss.data.cpu().numpy())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            train_loss += loss_val\n",
    "          train_loss /= train_size\n",
    "          test_loss = self.evaluate(train_data, valid_set)\n",
    "          scheduler.step()\n",
    "          print('Epoch %s Train Loss %s Test Loss %s' % (epoch, train_loss, test_loss))\n",
    "    \n",
    "    def evaluate(self, train_data, valid_set):\n",
    "\n",
    "        N, k = train_data.shape\n",
    "        X_feature_test, y_feature_test = valid_set\n",
    "\n",
    "        test_size = len(y_feature_test)\n",
    "        test_loss = 0\n",
    "\n",
    "        for i in range(test_size):\n",
    "\n",
    "            b = np.zeros((N, k))\n",
    "            if len(X_feature_test[i]) > 0:\n",
    "              selected_train_data = train_data[X_feature_test[i]]\n",
    "              b[:len(X_feature_test[i])] = selected_train_data\n",
    "\n",
    "            batch_X, batch_y = torch.FloatTensor(b).cuda(), torch.FloatTensor(y_feature_test[i:i+1]).cuda()\n",
    "            import pdb; pdb.set_trace()\n",
    "            batch_X, batch_y = batch_X.reshape((1, N, k)), batch_y.reshape((1, 1))\n",
    "            y_pred = self.model(batch_X)\n",
    "\n",
    "            loss = self.l2(y_pred, batch_y)\n",
    "            loss_val = np.asscalar(loss.data.cpu().numpy())\n",
    "            test_loss += loss_val\n",
    "        test_loss /= test_size\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "def array_to_lst(X_feature):\n",
    "\n",
    "  if type(X_feature) == list:\n",
    "    return X_feature\n",
    "\n",
    "  X_feature = list(X_feature)\n",
    "  for i in range(len(X_feature)):\n",
    "    X_feature[i] = X_feature[i].nonzero()[0]\n",
    "  return X_feature\n",
    "\n",
    "\n",
    "def findMostValuableSample_deepset_greedy(model, unlabeled_set, target_size):\n",
    "\n",
    "  model = model.cuda()\n",
    "\n",
    "  N, input_dim = unlabeled_set.shape\n",
    "  k = target_size\n",
    "\n",
    "  selected_subset = np.zeros(N)\n",
    "  selected_rank = []\n",
    "  selected_data = np.zeros((N, input_dim))\n",
    "\n",
    "  for i in range(k):\n",
    "    print(i)\n",
    "    maxIndex, maxVal = -1, -1\n",
    "    prevUtility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())\n",
    "    searchRange = np.where(selected_subset == 0)[0]\n",
    "    for j in searchRange:\n",
    "      selected_subset[j] = 1\n",
    "      selected_data[j] = unlabeled_set[j]\n",
    "      utility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())\n",
    "      selected_subset[j] = 0\n",
    "      selected_data[j] = np.zeros(input_dim)\n",
    "      if utility - prevUtility > maxVal:\n",
    "        maxIndex = j\n",
    "        maxVal = utility - prevUtility\n",
    "    selected_subset[maxIndex] = 1\n",
    "    selected_rank.append(maxIndex)\n",
    "    selected_data[maxIndex] = unlabeled_set[maxIndex]\n",
    "\n",
    "  return selected_subset, selected_rank, selected_data\n",
    "\n",
    "\n",
    "def findMostValuableSample_deepset_stochasticgreedy(model, unlabeled_set, target_size, epsilon, seed, verbose=False, debug=False, label=None):\n",
    "\n",
    "  model = model.cuda()\n",
    "\n",
    "  N, input_dim = unlabeled_set.shape\n",
    "  k = target_size\n",
    "\n",
    "  selected_subset = np.zeros(N)\n",
    "  selected_rank = []\n",
    "\n",
    "  R = int((N/k)*np.log(1/epsilon))\n",
    "  \n",
    "  print('Sample Size R={}'.format(R))\n",
    "\n",
    "  if debug:\n",
    "    R = 10\n",
    "    print('Sample Size R={}'.format(R))\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  selected_data = np.zeros((N, input_dim))\n",
    "\n",
    "  for i in range(k):\n",
    "    if verbose: print(i)\n",
    "    \n",
    "    maxIndex, maxVal = -1, -1\n",
    "\n",
    "    prevUtility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())\n",
    "    searchRange = np.where(selected_subset == 0)[0]\n",
    "\n",
    "    if debug:\n",
    "      print('Step {}, prevUtility={}'.format(i, prevUtility))\n",
    "\n",
    "    if R < len(searchRange):\n",
    "      searchRange = np.random.choice(searchRange, size=R, replace=False)\n",
    "\n",
    "    for j in searchRange:\n",
    "      selected_subset[j] = 1\n",
    "      selected_data[j] = unlabeled_set[j]\n",
    "      utility = model(torch.FloatTensor(selected_data.reshape((1, N, input_dim))).cuda())\n",
    "      selected_subset[j] = 0\n",
    "      selected_data[j] = np.zeros(input_dim)\n",
    "      gain = (utility - prevUtility).cpu().detach().numpy()[0][0]\n",
    "\n",
    "      if gain > maxVal:\n",
    "        maxIndex = j\n",
    "        maxVal = gain\n",
    "\n",
    "      if debug:\n",
    "        print('  Gain from {} is {}, label={}'.format(j, gain, label[j]))\n",
    "        print('  maxIndex={}, maxVal={}, labelMaxIndex={}'.format(maxIndex, maxVal, label[maxIndex]))\n",
    "      \n",
    "    selected_subset[maxIndex] = 1\n",
    "    selected_rank.append(maxIndex)\n",
    "    selected_data[maxIndex] = unlabeled_set[maxIndex]\n",
    "\n",
    "  return selected_subset, selected_rank, selected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oesTuGfx3G-9"
   },
   "source": [
    "## Add Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PvGYLdeD3GVq"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def addGaussianNoise(x_train, scale=1):\n",
    "  return np.clip(x_train+np.random.normal(scale=scale, size=x_train.shape), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCb1O55EcXXP"
   },
   "source": [
    "## CNN Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FwuYWEWtSolF"
   },
   "outputs": [],
   "source": [
    "class KerasLeNet:\n",
    "    @staticmethod\n",
    "    def build(numChannels, imgRows, imgCols, numClasses, activation=\"relu\", weightsPath=None):\n",
    "\n",
    "        model = Sequential()\n",
    "        inputShape = (imgRows, imgCols, numChannels)\n",
    "        if K.image_data_format() == \"channels_first\": \n",
    "          inputShape = (numChannels, imgRows, imgCols)\n",
    "\n",
    "        if numChannels > 1:\n",
    "          inputShape = (numChannels, imgRows, imgCols)\n",
    "\n",
    "        model.add(Conv2D(20, 5, padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(Conv2D(40, 5, padding=\"same\"))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(keras.layers.core.Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dense(numClasses))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        if weightsPath is not None:\n",
    "            model.load_weights(weightsPath)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class KerasLeNetUSPS:\n",
    "    @staticmethod\n",
    "    def build(numChannels, imgRows, imgCols, numClasses, activation=\"relu\", weightsPath=None):\n",
    "\n",
    "        model = Sequential()\n",
    "        inputShape = (imgRows, imgCols, numChannels)\n",
    "        if K.image_data_format() == \"channels_first\": \n",
    "          inputShape = (numChannels, imgRows, imgCols)\n",
    "\n",
    "        if numChannels > 1:\n",
    "          inputShape = (numChannels, imgRows, imgCols)\n",
    "\n",
    "        model.add(Conv2D(3, 5, padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(Conv2D(6, 5, padding=\"same\"))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(keras.layers.core.Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dense(numClasses))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        if weightsPath is not None:\n",
    "            model.load_weights(weightsPath)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def extractFeatures(layeredOutput, data):\n",
    "\n",
    "    featureDataList = list()\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        x = data[idx:idx+1]\n",
    "        featureVector = layeredOutput([x])[0][0]\n",
    "        dataList = featureVector.tolist()\n",
    "        featureDataList.append(dataList)\n",
    "\n",
    "    return np.array(featureDataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovDs3PC-UA7y",
    "outputId": "b9db966a-771b-4932-afcd-2e41f85d0db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7609 - accuracy: 0.4323 - val_loss: 0.5065 - val_accuracy: 0.8480\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.9088 - val_loss: 0.3581 - val_accuracy: 0.9003\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2085 - accuracy: 0.9414 - val_loss: 0.3228 - val_accuracy: 0.9143\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9596 - val_loss: 0.3095 - val_accuracy: 0.9203\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9597 - val_loss: 0.2858 - val_accuracy: 0.9233\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9658 - val_loss: 0.2488 - val_accuracy: 0.9387\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9706 - val_loss: 0.2461 - val_accuracy: 0.9377\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9761 - val_loss: 0.2389 - val_accuracy: 0.9402\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9796 - val_loss: 0.2287 - val_accuracy: 0.9442\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9800 - val_loss: 0.2323 - val_accuracy: 0.9442\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0543 - accuracy: 0.9828 - val_loss: 0.2229 - val_accuracy: 0.9482\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 0.9865 - val_loss: 0.2075 - val_accuracy: 0.9512\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9881 - val_loss: 0.2390 - val_accuracy: 0.9467\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.2247 - val_accuracy: 0.9507\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9901 - val_loss: 0.2175 - val_accuracy: 0.9527\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9886 - val_loss: 0.2359 - val_accuracy: 0.9512\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9900 - val_loss: 0.2111 - val_accuracy: 0.9562\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9929 - val_loss: 0.2300 - val_accuracy: 0.9522\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9943 - val_loss: 0.2434 - val_accuracy: 0.9502\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.9916 - val_loss: 0.2344 - val_accuracy: 0.9497\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9497\n",
      "[INFO] accuracy: 94.97%\n"
     ]
    }
   ],
   "source": [
    "x_train = usps_x_train.reshape((len(usps_x_train), 16, 16, 1))\n",
    "x_test = usps_x_test.reshape((len(usps_x_test), 16, 16, 1))\n",
    "\n",
    "y_train = np_utils.to_categorical(usps_y_train, 10)\n",
    "y_test = np_utils.to_categorical(usps_y_test, 10)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "model = KerasLeNetUSPS.build(numChannels=1, imgRows=16, imgCols=16, numClasses=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "(loss, accuracy) = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "# model.save_weights(save_dir+\"/usps_featureExtractor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r3C5yGGbNMG"
   },
   "outputs": [],
   "source": [
    "model_dir = 'drive/MyDrive/active-learning-transfer/models'\n",
    "model.save_weights(model_dir+\"/usps_featureExtractor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "sshtAXvcLabG",
    "outputId": "b0d6ea93-22e4-4b74-abab-e9d5835e00e9"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-750e2eed2804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasLeNetUSPS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumChannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgRows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/usps_featureExtractor.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlastLayerOp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2352\u001b[0m             'first, then load the weights.')\n\u001b[1;32m   2353\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m           \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/content/gdrive/My Drive//active-learning-transfer/models/usps_optimal/usps_featureExtractor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model_dir = prefix + '/active-learning-transfer/models/usps_optimal'\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "extractor = KerasLeNetUSPS.build(numChannels=1, imgRows=16, imgCols=16, numClasses=10)\n",
    "extractor.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "load_status = extractor.load_weights(model_dir+'/usps_featureExtractor.h5')\n",
    "lastLayerOp = K.function([extractor.layers[0].input], [extractor.layers[6].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgWFnqEwWAql"
   },
   "outputs": [],
   "source": [
    "x_train_few_cnnFeature = extractFeatures(lastLayerOp, usps_x_train_few.reshape((300, 16, 16, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrkxzmRaNO6i"
   },
   "source": [
    "# USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0TqRnvGNSSD"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import h5py \n",
    "from functools import reduce\n",
    "\n",
    "def hdf5(path, data_key = \"data\", target_key = \"target\", flatten = True):\n",
    "    \"\"\"\n",
    "        loads data from hdf5: \n",
    "        - hdf5 should have 'train' and 'test' groups \n",
    "        - each group should have 'data' and 'target' dataset or spcify the key\n",
    "        - flatten means to flatten images N * (C * H * W) as N * D array\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        X_tr = train.get(data_key)[:]\n",
    "        y_tr = train.get(target_key)[:]\n",
    "        test = hf.get('test')\n",
    "        X_te = test.get(data_key)[:]\n",
    "        y_te = test.get(target_key)[:]\n",
    "        if flatten:\n",
    "            X_tr = X_tr.reshape(X_tr.shape[0], reduce(lambda a, b: a * b, X_tr.shape[1:]))\n",
    "            X_te = X_te.reshape(X_te.shape[0], reduce(lambda a, b: a * b, X_te.shape[1:]))\n",
    "    return X_tr, y_tr, X_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAGx25IqOHTE"
   },
   "outputs": [],
   "source": [
    "usps_dir = prefix + '/active-learning-transfer/usps/usps.h5'\n",
    "\n",
    "usps_x_train, usps_y_train, usps_x_test, usps_y_test = hdf5(usps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f21hT74OdpA"
   },
   "outputs": [],
   "source": [
    "unlabel_size = 2000\n",
    "target_size = 1000\n",
    "\n",
    "np.random.seed(816)\n",
    "usps_unlabel_idx = np.random.choice(range(len(usps_x_train)), size=unlabel_size, replace=False)\n",
    "usps_x_unlabel, usps_y_unlabel = usps_x_train[usps_unlabel_idx], usps_y_train[usps_unlabel_idx]\n",
    "usps_x_unlabel[:1700] = addGaussianNoise(usps_x_unlabel[:1700], scale=10)\n",
    "\n",
    "x_unlabel_cnnFeature = extractFeatures(lastLayerOp, usps_x_unlabel.reshape((len(usps_x_unlabel), 16, 16, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJGXjpjDbg8t",
    "outputId": "2f14ae65-8e10-484c-c931-f9eac408def9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unlabel_cnnFeature.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5BFXxDzTHdf"
   },
   "source": [
    "### sample utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9pFTc0ITGby"
   },
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "X_feature, y_feature = sample_utility(n=5000, size_min=1, size_max=150, x_train=usps_x_train, y_train=usps_y_train, utility_func=uci_data_to_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxXu7Q-aT7Gy"
   },
   "source": [
    "### load utiltiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DbASp-3OlH_",
    "outputId": "dfbce964-d603-49d3-8f94-fe98830df3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 256)\n",
      "5000\n",
      "input_dim: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.018082459373399615 Test Loss 0.19594939464368788\n"
     ]
    }
   ],
   "source": [
    "sample_dir = prefix + '/active-learning-transfer/samples'\n",
    "\n",
    "(usps_x_train_few, usps_y_train_few), usps_X_feature, usps_y_feature = pickle.load( open(sample_dir+'/USPS_svm_N300_lo5_hi300_DataSeed803_Nsample5000_Seed1.sample', 'rb') )\n",
    "\n",
    "print(usps_x_train_few.shape)\n",
    "print(len(usps_X_feature))\n",
    "\n",
    "x_train_few_cnnFeature = extractFeatures(lastLayerOp, usps_x_train_few.reshape((300, 16, 16, 1)))\n",
    "\n",
    "usps_y_feature = np.array(usps_y_feature)\n",
    "usps_X_feature_test, usps_y_feature_test = usps_X_feature[4000:], usps_y_feature[4000:]\n",
    "usps_X_feature, usps_y_feature = usps_X_feature[:4000], usps_y_feature[:4000]\n",
    "\n",
    "\n",
    "\n",
    "input_dim = x_train_few_cnnFeature.shape[1]\n",
    "print('input_dim:', input_dim)\n",
    "set_features=128\n",
    "hidden_ext=128\n",
    "hidden_reg=128\n",
    "n_epoch=50\n",
    "batch_size=32\n",
    "lr = 1e-4\n",
    "\n",
    "model_usps = Utility_deepset_dual(model=DeepSet_dual(input_dim*2, set_features, hidden_ext, hidden_reg))\n",
    "\n",
    "for e in range(n_epoch):\n",
    "  model_usps.fit(x_train_few_cnnFeature, (usps_X_feature, usps_y_feature), (usps_X_feature_test, usps_y_feature_test), 1, batch_size, lr,  n_pair=20000)\n",
    "  # _, pair_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_usps.model, x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=101)\n",
    "  rank_func = findMostValuableSample_dual(model_usps.model, x_unlabel_cnnFeature, target_size, n_sample=100)\n",
    "  pair_selected_rank = rank_func.get_rank()\n",
    "  print(sum(np.array(pair_selected_rank[0]) < 1700))\n",
    "  print(sum(np.array(pair_selected_rank[1]) < 1700))\n",
    "  print(sum(np.array(pair_selected_rank[2]) < 1700))\n",
    "  print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "aMtClkA6aQIw",
    "outputId": "a92c60cd-397d-44ed-b627-7a5cea82620d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 256)\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-59-f11ed7ab18bb>(97)evaluate()\n",
      "-> batch_X, batch_y = batch_X.reshape((1, N, k)), batch_y.reshape((1, 1))\n",
      "(Pdb) print(batch_X.shape)\n",
      "torch.Size([300, 96])\n",
      "(Pdb) print(train_data.shape)\n",
      "(300, 96)\n",
      "(Pdb) quit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-a6da130a6a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel_optimal_usps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtility_deepset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeepSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mmodel_optimal_usps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_few_cnnFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musps_X_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musps_y_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musps_X_feature_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musps_y_feature_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_dulo_selected_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindMostValuableSample_deepset_stochasticgreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_optimal_usps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_unlabel_cnnFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_dulo_selected_rank\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-f11ed7ab18bb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, train_set, valid_set, n_epoch, batch_size, lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m           \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m           \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s Train Loss %s Test Loss %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-f11ed7ab18bb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, train_data, valid_set)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_feature_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-f11ed7ab18bb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, train_data, valid_set)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_feature_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_dir = prefix + '/active-learning-transfer/samples'\n",
    "\n",
    "(usps_x_train_few, usps_y_train_few), usps_X_feature, usps_y_feature = pickle.load( open(sample_dir+'/USPS_svm_N300_lo5_hi300_DataSeed803_Nsample5000_Seed1.sample', 'rb') )\n",
    "\n",
    "print(usps_x_train_few.shape)\n",
    "print(len(usps_X_feature))\n",
    "\n",
    "x_train_few_cnnFeature = extractFeatures(lastLayerOp, usps_x_train_few.reshape((300, 16, 16, 1)))\n",
    "\n",
    "usps_y_feature = np.array(usps_y_feature)\n",
    "usps_X_feature_test, usps_y_feature_test = usps_X_feature[4000:], usps_y_feature[4000:]\n",
    "usps_X_feature, usps_y_feature = usps_X_feature[:4000], usps_y_feature[:4000]\n",
    "\n",
    "\n",
    "\n",
    "input_dim = x_train_few_cnnFeature.shape[1]\n",
    "set_features=128\n",
    "hidden_ext=128\n",
    "hidden_reg=128\n",
    "n_epoch=50\n",
    "batch_size=32\n",
    "lr = 1e-4\n",
    "\n",
    "model_optimal_usps = Utility_deepset(model=DeepSet(input_dim, set_features, hidden_ext, hidden_reg))\n",
    "for e in range(n_epoch):\n",
    "  model_optimal_usps.fit(x_train_few_cnnFeature, (usps_X_feature, usps_y_feature), (usps_X_feature_test, usps_y_feature_test), 1, batch_size, lr)\n",
    "  _, optimal_dulo_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_optimal_usps.model, x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=101)\n",
    "  print(sum(np.array(optimal_dulo_selected_rank) < 1700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q8Ln9BKPyV4"
   },
   "outputs": [],
   "source": [
    "torch.save(model_optimal_usps.model.state_dict(), model_dir+'/usps_optimal.state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTYVmjaqrtAu"
   },
   "source": [
    "### Run Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "A3xuIlY6ruGN",
    "outputId": "c64c7786-b3e5-4845-965e-712ecec66471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size R=18\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-13c063227860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel_optimal_usps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/usps_optimal_new.state_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_dulo_selected_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindMostValuableSample_deepset_stochasticgreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_optimal_usps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_unlabel_cnnFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_dulo_selected_rank\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e18409fe12ba>\u001b[0m in \u001b[0;36mfindMostValuableSample_deepset_stochasticgreedy\u001b[0;34m(model, unlabeled_set, target_size, epsilon, seed, verbose, debug, label)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmaxIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mprevUtility\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0msearchRange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_subset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fcc98cd2118b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "prefix = '/content/gdrive/My Drive/'\n",
    "model_dir = prefix + 'active-learning-transfer/models/usps_optimal'\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "extractor = KerasLeNetUSPS.build(numChannels=1, imgRows=16, imgCols=16, numClasses=10)\n",
    "extractor.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "load_status = extractor.load_weights(model_dir+'/usps_featureExtractor.h5')\n",
    "lastLayerOp = K.function([extractor.layers[0].input], [extractor.layers[6].output])\n",
    "\n",
    "usps_dir = prefix + 'active-learning-transfer/usps/usps.h5'\n",
    "\n",
    "usps_x_train, usps_y_train, usps_x_test, usps_y_test = hdf5(usps_dir)\n",
    "\n",
    "unlabel_size = 2000\n",
    "target_size = 1000\n",
    "\n",
    "np.random.seed(816)\n",
    "usps_unlabel_idx = np.random.choice(range(len(usps_x_train)), size=unlabel_size, replace=False)\n",
    "usps_x_unlabel, usps_y_unlabel = usps_x_train[usps_unlabel_idx], usps_y_train[usps_unlabel_idx]\n",
    "usps_x_unlabel[:1700] = addGaussianNoise(usps_x_unlabel[:1700], scale=10)\n",
    "\n",
    "x_unlabel_cnnFeature = extractFeatures(lastLayerOp, usps_x_unlabel.reshape((len(usps_x_unlabel), 16, 16, 1)))\n",
    "\n",
    "# input_dim = x_unlabel_cnnFeature.shape[1]\n",
    "input_dim = 500\n",
    "set_features=128\n",
    "hidden_ext=128\n",
    "hidden_reg=128\n",
    "n_epoch=50\n",
    "batch_size=32\n",
    "lr = 1e-4\n",
    "\n",
    "model_optimal_usps = Utility_deepset(model=DeepSet(input_dim, set_features, hidden_ext, hidden_reg))\n",
    "\n",
    "model_optimal_usps.model.load_state_dict(torch.load(model_dir+'/usps_optimal_new.state_dict'))\n",
    "\n",
    "_, optimal_dulo_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_optimal_usps.model, x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=101)\n",
    "\n",
    "print(sum(np.array(optimal_dulo_selected_rank) < 1700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gw28hIwj242V",
    "outputId": "e6b200a9-2541-479e-b122-bb578f266820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(optimal_dulo_selected_rank))\n",
    "np.savetxt(model_dir+\"/optimal_dulo_selected_rank_seed816_1000_new.csv\", optimal_dulo_selected_rank, delimiter =\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qhi0ypv7cUf",
    "outputId": "34c678f2-082e-4265-da01-5ed8f57f55ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1333, 1712, 1948, 1704, 1295, 1878, 1820, 1886, 1900, 225, 1761, 1717, 1858, 328, 1293, 803, 691, 1777, 1921, 1997, 1548, 1961, 135, 1819, 1883, 1809, 1978, 1703, 1731, 1938, 1936, 1556, 693, 642, 1784, 812, 1985, 1729, 1987, 1930, 1905, 1805, 1898, 542, 1262, 1786, 246, 1723, 1984, 1783, 1956, 161, 1846, 1932, 1916, 1764, 1744, 1554, 1973, 1903, 1633, 1859, 1532, 1839, 1267, 1754, 1890, 1877, 448, 1829, 1899, 1862, 1995, 1982, 1871, 103, 575, 873, 1812, 1254, 1604, 1887, 1792, 1988, 1707, 1832, 1516, 1714, 1747, 1813, 1843, 1621, 1702, 1957, 1048, 239, 1725, 1855, 1866, 763, 1966, 1101, 451, 1914, 1826, 368, 618, 1710, 1852, 1773, 1976, 256, 1571, 478, 1849, 307, 1884, 1928, 1776, 1864, 1983, 734, 126, 1746, 450, 1891, 1685, 1771, 1741, 1711, 990, 679, 780, 1210, 1050, 1221, 1944, 1721, 262, 1906, 1857, 489, 1872, 855, 1724, 1889, 224, 1845, 1152, 1263, 1191, 484, 1759, 1986, 1909, 1781, 680, 1865, 1825, 927, 245, 1880, 1919, 1824, 1807, 406, 1403, 1748, 1827, 919, 1965, 1933, 259, 1833, 1701, 1992, 1031, 546, 1762, 1931, 1791, 1917, 128, 1841, 713, 1240, 1927, 547, 576, 1811, 1478, 1726, 560, 1946, 1767, 1772, 1231, 1907, 1808, 1847, 1993, 1163, 56, 1885, 862, 1970, 1169, 1929, 383, 1057, 147, 1940, 773, 1962, 443, 1818, 1869, 532, 1765, 1575, 1787, 1719, 1448, 1021, 986, 1684, 1367, 905, 1980, 1994, 891, 223, 1718, 1920, 1077, 1969, 1187, 1981, 1326, 1503, 1801, 1737, 1834, 1823, 1769, 1991, 1842, 1705, 1457, 1947, 1749, 1799, 1766, 1410, 1774, 652, 854, 737, 1941, 1037, 1974, 1785, 298, 1742, 1850, 1093, 675, 866, 129, 1757, 339, 697, 1502, 449, 1523, 1945, 1935, 1500, 1896, 1763, 255, 1861, 1804, 1816, 381, 1996, 384, 1129, 1960, 327, 538, 665, 498, 1560, 1963, 1830, 1100, 1590, 364, 1854, 1736, 586, 1989, 402, 1837, 123, 1955, 439, 565, 767, 1760, 1106, 601, 536, 1888, 1330, 1851, 152, 720, 1882, 1029, 1924, 765, 424, 596, 1750, 1953, 548, 34, 1030, 1950, 1727, 1722, 1310, 964, 1113, 1967, 1836, 420, 1427, 603, 1768, 983, 1060, 552, 1868, 1901, 1161, 1951, 1445, 1873, 1835, 1918, 1802, 742, 1893, 57, 1709, 1806, 36, 1273, 1923, 1639, 1063, 1867, 366, 12, 156, 1320, 1942, 724, 1964, 1971, 1026, 240, 172, 1203, 1894, 1618, 973, 799, 1708, 1775, 1386, 1324, 1958, 885, 76, 923, 518, 1328, 1346, 101, 1716, 1870, 1803, 1700, 1720, 1840, 51, 468, 1584, 1972, 655, 1204, 1591, 422, 1908, 1319, 1875, 29, 81, 1853, 572, 211, 1632, 17, 893, 64, 1856, 761, 1592, 1613, 346, 991, 1322, 1479, 1949, 1904, 1795, 341, 1977, 755, 1745, 567, 1189, 1649, 1583, 579, 584, 1959, 1735, 1663, 903, 1138, 171, 1822, 1863, 269, 1796, 1363, 1756, 1335, 1390, 288, 1096, 739, 1751, 1305, 673, 486, 738, 797, 968, 1615, 1572, 136, 336, 1686, 1078, 1881, 1998, 85, 1183, 133, 84, 1524, 704, 330, 940, 1828, 922, 1815, 513, 1582, 320, 226, 1821, 1019, 159, 1706, 83, 1952, 35, 444, 795, 483, 1596, 640, 167, 1897, 80, 1151, 1459, 1435, 375, 437, 821, 1874, 941, 337, 1563, 1046, 492, 1365, 1635, 62, 1753, 1666, 1477, 871, 1879, 1619, 1150, 1789, 537, 1788, 39, 597, 195, 79, 198, 945, 1734, 1943, 1902, 1506, 473, 1536, 898, 1028, 1922, 636, 1770, 13, 613, 1401, 1133, 952, 1264, 754, 1580, 1015, 1910, 252, 1876, 151, 1219, 1012, 333, 1172, 730, 15, 1, 438, 1655, 1529, 326, 611, 1780, 1730, 1954, 741, 1620, 729, 144, 1411, 53, 820, 1794, 853, 1848, 683, 609, 1601, 68, 557, 1689, 900, 1728, 1134, 1913, 1488, 1304, 778, 1171, 1020, 1396, 1559, 22, 1428, 1810, 309, 1493, 1482, 48, 517, 1444, 1676, 1115, 390, 1642, 187, 1200, 971, 1912, 476, 1176, 1798, 1086, 192, 1646, 757, 1042, 1248, 705, 844, 202, 859, 219, 934, 996, 1778, 1074, 1049, 1094, 1196, 856, 1058, 1895, 810, 1599, 335, 647, 165, 549, 1181, 992, 1311, 285, 310, 185, 600, 1968, 1441, 562, 556, 348, 283, 998, 1177, 74, 71, 1075, 134, 1278, 295, 345, 1294, 196, 465, 1740, 509, 132, 723, 1755, 1425, 1124, 59, 961, 232, 26, 908, 786, 1193, 1157, 685, 1743, 1053, 503, 43, 1715, 1589, 920, 452, 1512, 798, 967, 124, 120, 1099, 1911, 1562, 633, 1460, 431, 1576, 55, 91, 1337, 409, 75, 394, 1364, 1934, 100, 635, 1385, 469, 72, 1450, 370, 766, 1926, 776, 1480, 1224, 434, 441, 608, 774, 915, 455, 561, 717, 1925, 1051, 794, 47, 200, 82, 1332, 700, 784, 1573, 1471, 791, 1544, 662, 782, 604, 1027, 496, 174, 573, 1288, 1379, 732, 1432, 768, 230, 6, 1141, 726, 247, 1638, 1567, 286, 221, 102, 515, 106, 1617, 25, 1317, 1184, 522, 1662, 376, 884, 835, 1539, 148, 970, 480, 1195, 183, 678, 595, 627, 146, 892, 1062, 667, 702, 1415, 1395, 1117, 849, 1389, 1007, 1340, 1817, 340, 107, 607, 488, 1422, 367, 1061, 1143, 1483, 109, 314, 282, 612, 1041, 1519, 882, 1064, 1329, 1814, 458, 1504, 1132, 913, 1790, 207, 470, 721, 979, 1281, 37, 201, 935, 1696, 781, 1937, 1545, 1629, 749, 354, 664, 296, 1002, 670, 1218, 1023, 711, 5, 686, 1246, 1382, 1831, 1179, 577, 1378, 389, 188, 1130, 1605, 790, 838, 1436, 1623, 833, 1644, 1146, 1574, 555, 582, 197, 846, 1462, 590, 387, 759, 19, 392, 1230, 212, 521, 1384, 1116, 380, 1111, 42, 1690, 284, 1229, 1446, 511, 1283, 836, 976, 974, 1056, 718, 1595, 1499, 1593, 939, 1268, 1414, 479, 0, 1065, 955, 213, 804, 907, 1039, 398, 1498, 663, 1126, 1147, 342, 1682, 155, 756, 193, 1664, 65, 1699, 254, 840, 4, 972, 410, 1238, 796, 1508, 446, 191, 1321, 743, 1677, 1610, 746, 1975, 388, 1844, 70, 442, 32, 1640, 1188, 792, 209, 274, 1697, 914, 270, 1485, 297, 1838, 1342, 1670, 1104, 250, 889, 1494, 623, 14, 554, 1408, 1637, 932, 879, 694, 502, 1678, 906, 1269, 1198, 943, 1630, 1292, 988, 965, 626, 1296, 1528, 1297, 97, 396, 1653, 783, 1402, 294, 775, 1496, 1647, 227, 1331, 1355, 1668, 322, 1256]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_dulo_selected_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9LgwmOy4F93",
    "outputId": "6920d411-b8d4-49e0-8c07-89dcce80e2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/active-learning-transfer/models/usps_optimal\n"
     ]
    }
   ],
   "source": [
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpxrvb192T6s",
    "outputId": "ea09f03c-cc22-492f-e41c-acfdf98279d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L8Kp80j027n"
   },
   "outputs": [],
   "source": [
    "model_dir = prefix + '/active-learning-transfer/models/cycada/ds/iter_1_28_noise'\n",
    "adda_net_file = model_dir + '/adda_LeNet_net_mnist_usps.pth'\n",
    "net = get_model('AddaNet', num_cls=10, weights_init=adda_net_file, \n",
    "                model='LeNet')\n",
    "net.eval()\n",
    "\n",
    "def featureExtract_cycda(x, ext):\n",
    "  assert list(x.shape[1:]) == [1, 28, 28]\n",
    "  score, out = ext(x.cuda(), with_ft=True)\n",
    "  return out.view(out.size(0), -1)\n",
    "\n",
    "prefix = '/content/gdrive/My Drive/'\n",
    "usps_dir = prefix + 'active-learning-transfer/usps/usps.h5'\n",
    "usps_x_train, usps_y_train, usps_x_test, usps_y_test = hdf5(usps_dir)\n",
    "\n",
    "unlabel_size = 2000\n",
    "target_size = 1000\n",
    "\n",
    "np.random.seed(816)\n",
    "usps_unlabel_idx = np.random.choice(range(len(usps_x_train)), size=unlabel_size, replace=False)\n",
    "usps_x_unlabel, usps_y_unlabel = usps_x_train[usps_unlabel_idx], usps_y_train[usps_unlabel_idx]\n",
    "usps_x_unlabel[:1700] = addGaussianNoise(usps_x_unlabel[:1700], scale=10)\n",
    "\n",
    "usps_x_unlabel_re, usps_y_unlabel_re, usps_x_test_re, usps_y_test_re = usps_encoderProcess(usps_x_unlabel, usps_y_unlabel, usps_x_test, usps_y_test)\n",
    "x_unlabel_cnnFeature = featureExtract_cycda(usps_x_unlabel_re, net.tgt_net)\n",
    "x_unlabel_cnnFeature = x_unlabel_cnnFeature.cpu().detach().numpy()\n",
    "\n",
    "input_dim = x_unlabel_cnnFeature.shape[1]\n",
    "set_features=128\n",
    "hidden_ext=128\n",
    "hidden_reg=128\n",
    "n_epoch=50\n",
    "batch_size=32\n",
    "lr = 1e-4\n",
    "\n",
    "model_optimal_usps = Utility_deepset(model=DeepSet(input_dim, set_features, hidden_ext, hidden_reg))\n",
    "\n",
    "model_optimal_usps.model.load_state_dict(torch.load(model_dir+'/usps_optimal.state_dict'))\n",
    "\n",
    "_, optimal_dulo_selected_rank, _ = findMostValuableSample_deepset_stochasticgreedy(model_optimal_usps.model, x_unlabel_cnnFeature, target_size, epsilon=1e-4, seed=101)\n",
    "\n",
    "print(sum(np.array(optimal_dulo_selected_rank) < 1700))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "USPS-multi.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
